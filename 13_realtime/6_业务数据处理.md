# 实时数仓（二）---业务数据处理

***



## 1. 业务数据处理架构

![1594980972405](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233018.png)

> 业务数据, 比如用户的订单,支付等操作会存储在 Mysql 中. 为便于 SparkStreaming 对这些业务数据实时分析处理, 这些数据一般也会再存储到 Kafka 中.



## 2. Canal的使用

### 2.1 Canal概述

从 Mysql 到 Kafka 的过程中, 如果每次都是全表扫描进行数据的转移, 则非常耗时, 并且也会对 Mysql 造成性能的影响.

最好的办法是使用专门的工具能够实时的监控 Mysql 数据的变化.

Canal 就是一个我们想要的工具.

**Canal 的作用就是实时同步 Mysql**

### 2.2 Canal工作原理

#### 2.2.1 MySql主从复制机制

##### 主从复制原理

![1594981217507](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233015.png)

MySQL复制过程分成三步：
1 master将改变记录到**二进制日志（binary log）**。这些**记录过程叫做二进制日志事件**，binary log events；
2 slave将master的binary log events拷贝到它的中继日志（relay log）；
3 slave重做中继日志中的事件，将改变应用到自己的数据库中。 **MySQL复制是异步的且串行化的**

> Q：如何理解MySQL的复制是异步的
>
> MySQL的复制机制是异步的是说当客户端往主库中插入数据后，**只要主库接收数据后持久化到磁盘上，保证了数据的安全性后就返回给客户端确认相应**。**而从库数据有没有复制，数据复制有没有成功，客户端是不关心的**。比如说你的应用程序写入数据是到主库的，而查询数据是从从库查询的，那么就可能会出现查询不到数据的结果。因为从库不一定会那么快从主库把数据读取过来，或者复制数据失败，这就是异步带来的不一致性。而同步就是客户端往主库插入数据，直到从库把数据安全复制过来之后才会返回结果给客户端。可想而知，异步带来的是性能的提升，而同步会降低数据的写入效率。

##### 复制的原则

- 每个slave只有一个master
- 每个slave只能有一个唯一的服务器ID
- 每个master可以有多个salve

#### 2 2.2 Binary log的理解

MySQL的二进制日志可以说是 MySQL 最重要的日志了，它记录了所有的 DDL 和DML (除了数据查询语句)语句，以事件形式记录，还包含语句所执行的消耗的时间，MySQL的二进制日志是事务安全型的。

一般来说开启 binlog 日志大概会有 1% 的性能损耗。 binlog日志有两个最重要的使用场景:

1. **主从复制**，MySQL Replication 在 Master 端开启 binlog，Mster 把它的二进制日志传递给 slaves 来达到 master-slave 数据一致的目的。

2. **数据恢复**，通过使用 mysql binlog 工具来使恢复数据。

##### binlog格式

binlog 有 3 种格式: STATEMENT, ROW, MIXED

1. **statement**

语句级别, binlog 会记录每次执行的写操作的语句, 注意记录的是语句, slave 会自己重新执行写操作的语句, 从而达到与 master 的一致.

但是有可能会出现主从不一致的情况: 比如存储当前时间戳, 存储一个随机值等.

- 优点： 节省空间

- 缺点： 有可能造成数据不一致。

2. **row**

行级， binlog 会记录每次操作后每行记录的变化。

- 优点：保持数据的绝对一致性。因为不管sql是什么，引用了什么函数，他只记录执行后的效果。

- 缺点：占用较大空间。 如果一条语句执行之后导致很多行发生了变化, 则会产生很多条记录

3. **mixed**

  statement 的升级版，一定程度上解决了因为一些情况而造成的 statement模式不一致问题

- 在某些情况下会按照ROW的方式进行处理

```
 1. 当函数中包含 UUID() 时
 2. 包含 AUTO_INCREMENT 字段的表被更新时
 3. 执行 INSERT DELAYED 语句时；
 4. 用 UDF 时； 
```

- 优点：节省空间，同时兼顾了一定的一致性。
- 缺点：还有些极个别情况依旧会造成不一致，另外statement和mixed对于需要对binlog 的监控的情况都不方便。

> 由于 canal 是监控的数据的变化, 所以 binlog 的格式需要设置成 row 格式

#### 2.2.3 Canal工作原理

Canal 的工作原理很简单， 就是把自己伪装成 slave，假装从 master 复制数据 。

![1594982449078](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233039.png)

> 1. canal模拟mysql slave的交互协议，伪装自己为mysql slave，向mysql master发送dump协议
>
> 2. mysql master收到dump请求，开始推送 binary log 给 slave(也就是 canal)
>
> 3. canal 解析 binary log 对象(原始为 byte 流)

### 2.3 MySQL配置

#### 2.3.1 赋予权限

赋权限(可以省略, 后面都是使用的root用户)

```sql
GRANT ALL PRIVILEGES ON *.* TO canal@'%' IDENTIFIED BY 'canal';
```

#### 2.3.2 开启binlog

打开文件/etc/my.cnf, 如果没有就创建一个

```properties
[mysqld]
server-id= 1
log-bin= mysql-bin
binlog_format= row

```

#### 2.3.3 重启mysql使binlog生效

```shell
sudo systemctl restart mysqld
```

检查 binlog 是否生效

![1594982896903](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233046.png)

#### 2.3.4 数据准备

> 注意该步不是主从配置的必要配置，只是该项目的数据准备

- 创建数据库

```sql
create database gmall DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
```

![1594982990975](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233054.png)

- 执行脚本
- 数据插入

上个步骤 中的 sql 脚本执行完毕之后, 部分表中已经插入了数据, 但是和订单相关的表中没有数据. 可以执行存储过程来插入数据.

```sql
# 日期  订单个数 用户数 是否删除以前的数据
call init_data("2019-05-16", 10,2,false)
```

> 该操作模拟了数据生成到mysql的步骤，后续通过该命令手动给mysql数据库添加业务数据后完成下一步的操作

### 2.4 Canal部署

#### 2.4.1 canal下载

```shell
wget https://github.com/alibaba/canal/releases/download/canal-1.1.2/canal.deployer-1.1.2.tar.gz
```

#### 2.4.2 解压

```shell
mkdir /opt/module/canal
tar -zxvf canal.deployer-1.1.2.tar.gz -C /opt/module/canal
```

#### 2.4.3 配置

##### 通用配置

**conf/canal.properties canal 的通用配置, 主要关注下canal.port, 默认是11111**

![1594983169281](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233102.png)

> canal.port：canal客户端与canal连接的默认端口号

##### 实例配置

**conf/example/instance.properties instance.properties是针对要追踪的 mysql 的实例配置**

```properties
#################################################
## mysql serverId , v1.0.26+ will autoGen
# slaveId 必须配置, 不能和 mysql 的 id 重复
canal.instance.mysql.slaveId=100

# enable gtid use true/false
canal.instance.gtidon=false

# position info
# mysql 的位置信息
canal.instance.master.address=hadoop201:3306
canal.instance.master.journal.name=
canal.instance.master.position=
canal.instance.master.timestamp=
canal.instance.master.gtid=

# rds oss binlog
canal.instance.rds.accesskey=
canal.instance.rds.secretkey=
canal.instance.rds.instanceId=

# table meta tsdb info
canal.instance.tsdb.enable=true
#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb
#canal.instance.tsdb.dbUsername=canal
#canal.instance.tsdb.dbPassword=canal

#canal.instance.standby.address =
#canal.instance.standby.journal.name =
#canal.instance.standby.position =
#canal.instance.standby.timestamp =
#canal.instance.standby.gtid=

# username/password
# 用户名和密码
canal.instance.dbUsername=root
canal.instance.dbPassword=aaa
canal.instance.connectionCharset = UTF-8
canal.instance.defaultDatabaseName =
# enable druid Decrypt database password
canal.instance.enableDruid=false
#canal.instance.pwdPublicKey=MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBALK4BUxdDltRRE5/zXpVEVPUgunvscYFtEip3pmLlhrWpacX7y7GCMo2/JM6LeHmiiNdH1FWgGCpUfircSwlWKUCAwEAAQ==

# table regex
# db.table
canal.instance.filter.regex=.*\\..*
# table black regex
canal.instance.filter.black.regex=

# mq config
canal.mq.topic=example
canal.mq.partition=0
# hash partition config
#canal.mq.partitionsNum=3
#canal.mq.partitionHash=mytest.person:id,mytest.role:id
#################################################

```

> canal.instance.mysql.slaveId 不能和mysql主机相同
>
> canal.instance.master.address：master的地址
>
> canal.instance.dbUsername/canal.instance.dbPassword：连接主机的验证信息
>
> canal.instance.defaultDatabaseName：默认不写
>
> canal.instance.filter.regex：以正则表达式说明要追踪哪些数据库下的哪些表
>
> ```
> 在java中 . ---> 匹配任意一个字符  如果需要一个普通的 . 需要加\.
> 		\ ---> 如果需要一个普通\，需要加\，即\\
> .*\\..*:表示追踪任意库下的任意表
> ```
>
> 

#### 2.4.4 启停与查看

```shell
#启动
bin/startup.sh
#停止
bin/stop.sh

#查看
[atguigu@hadoop109 logs]$ ll
总用量 8
drwxrwxr-x 2 atguigu atguigu 4096 7月  17 10:40 canal
drwxrwxr-x 2 atguigu atguigu 4096 7月  17 12:50 example

```

> ![1594983308873](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233111.png)
>
> 

## 3. 项目工程部分

业务数据处理部分仍从属于实时数仓一中的大项目中，属于项目的一个module，依赖关系需要继承于父工程

### 3.1 父工程依赖

```xml
...
    <modules>
        <module>gmall-common</module>
        <module>gmall-mock</module>
        <module>gmall-logger</module>
        <module>gmall-realtime</module>
        <module>gmall-publisher</module>
        <module>gmall-canal</module>
    </modules>
...
```

## 4 数据采集部分-canal

功能：完成从 Canal 中读取数据, 然后发送的 Kafka 中

模块名:`gmall-canal`

![1594988340781](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233118.png)

### 4.1 canal读取数据

#### 4.1.1 依赖添加

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>realtime-gmall</artifactId>
        <groupId>com.orange.lin</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.orange.lin</groupId>
    <artifactId>gmall-canal</artifactId>
    <version>1.0-SNAPSHOT</version>
    <dependencies>
        <!-- https://mvnrepository.com/artifact/com.alibaba.otter/canal.client -->
        <!--canal 客户端, 从 canal 服务器读取数据-->
        <dependency>
            <groupId>com.alibaba.otter</groupId>
            <artifactId>canal.client</artifactId>
            <version>1.1.2</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients -->
        <!-- kafka 客户端 -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>2.4.1</version>
        </dependency>
        <dependency>
            <groupId>com.orange.lin</groupId>
            <artifactId>gmall-common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
    </dependencies>


</project>
```

#### 4.1.2 kafka生产者工具类

```scala
package com.orange.lin

import java.util.Properties

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-14:02
 */
object MyKafkaUtil {
  val pros = new Properties()
  pros.setProperty("bootstrap.servers","hadoop109:9092,hadoop110:9092,hadoop111:9092")
  pros.setProperty("key.serializer","org.apache.kafka.common.serialization.StringSerializer")
  pros.setProperty("value.serializer","org.apache.kafka.common.serialization.StringSerializer")
  val producer = new KafkaProducer[String,String](pros)
  /*
  spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
   */
  def sendToKafka(topic:String,content:String)={
      producer.send(new ProducerRecord[String,String](topic,content))

  }

}

```



### 4.2 canal读取客户端并发送数据到Kafka

- 客户端读取数据的流程
  1. 创建连接cannal的对象
  2. 连接canal
  3. 订阅数据
  4. 解析数据（从canal对数据的包装类中解析出数据）
  5. 继续回到第四步，重复获取数据（实时框架一般不会停止

```scala
package com.orange.lin

import java.net.InetSocketAddress
import java.util

import com.alibaba.fastjson.JSONObject
import com.alibaba.otter.canal.client.{CanalConnector, CanalConnectors}
import com.alibaba.otter.canal.protocol.CanalEntry.{EntryType, EventType, RowChange}
import com.alibaba.otter.canal.protocol.{CanalEntry, Message}
import com.google.protobuf.ByteString

import scala.collection.JavaConverters._
/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-10:51
 */
object CanalClient {

  def handleData(rowDatas: util.List[CanalEntry.RowData],
                 tableName: String,
                 eventType: CanalEntry.EventType) = {

    if (tableName=="order_info"&&
      eventType==EventType.INSERT&&
      rowDatas!=null
      &&rowDatas.size()>0){
      // 变化后的列
      //mysql中的一行数据 对应kafka中的 一条
        for (rowData<-rowDatas.asScala){
          val obj = new JSONObject()
          val columnsList: util.List[CanalEntry.Column] = rowData.getAfterColumnsList
          for (column<-columnsList.asScala){
            // id: 100  total_amount: 1000.2
            val key=column.getName
            val value=column.getValue
            obj.put(key,value)
          }
//          println(obj.toJSONString) //test
          // 4. 解析后的数据, 组成json字符串, 写入到kafka
          MyKafkaUtil.sendToKafka(Constant.ORDER_INFO_TOPIC,obj.toJSONString)
        }
    }
  }

  def main(args: Array[String]): Unit = {


    // 1. 连接canal
    val connector: CanalConnector = CanalConnectors.newSingleConnector(
      new InetSocketAddress("hadoop109", 11111),
      "example",
      "",
      "")
    // 连接canal服务器
    connector.connect()
    // 2. 拉取数据
    // 2.1 订阅想读的数据
    connector.subscribe("gmall0213.*")
    // 2.2 拉取
    while (true) {
      // 100表示最多拉取由于100条sql导致变化的数据.
      // 所有的数据封装到一个Message中
      val message: Message = connector.get(100)
      // 3. 解析数据
      val entries: util.List[CanalEntry.Entry] = message.getEntries

      //解析数据

      if (entries.size() > 0) {

        for (entry <- entries.asScala) {
          //entry类型必须是行变化
          // entry的类型必须是ROWDATA
          if (entry != null &&
            entry.hasEntryType &&
            entry.getEntryType == EntryType.ROWDATA) {

            val value: ByteString = entry.getStoreValue
            val rowChange: RowChange = RowChange.parseFrom(value)
            // 所有行变化的数据
            val rowDatasList: util.List[CanalEntry.RowData] = rowChange.getRowDatasList

            handleData(rowDatasList,entry.getHeader.getTableName,rowChange.getEventType)
          }
        }
      } else { //没有拉取到数据
        println("没有抓取到数据......,3S之后重新抓取")
        Thread.sleep(3000)// 休眠3秒后继续拉取数据
      }
    }
  }

}

```

> ![1594988792335](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233129.png)
>
> StoreValue是Entry的一个属性
>
> RowChange：多行数据的变化，不包含事务、心跳等变化
>
> RowData：一行数据，里面有列信息
>
> 列信息里：有列名和列值



## 5 实时处理部分-- SparkStreaming（重要）

![1594990105699](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233136.png)

该部分从属于实时数仓一中的实时处理模块，属于同一module下的多个指标

### 5.1 从Kafka消费流

```scala
package com.orange.lin.app
import com.alibaba.fastjson.JSON
import com.orange.lin.Constant
import com.orange.lin.bean.OrderInfo
import com.orange.lin.util.MyKafkaUtil
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-15:56
 */
object  OrderApp extends BaseApp {
  override def run(ssc: StreamingContext): Unit = {
    //获取流
      val ds: DStream[String] = MyKafkaUtil.getKafkaStream(ssc,Constant.ORDER_INFO_TOPIC)


	...
	...
  }
}

```

### 5.2 数据结构转换与脱敏

#### 样例类的生成

```scala
package com.orange.lin.bean

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-16:06
 */
case class OrderInfo(id: String,
                     province_id: String,
                     var consignee: String,
                     order_comment: String,
                     var consignee_tel: String,
                     order_status: String,
                     payment_way: String,
                     user_id: String,
                     img_url: String,
                     total_amount: Double,
                     expire_time: String,
                     delivery_address: String,
                     create_time: String,
                     operate_time: String,
                     tracking_no: String,
                     parent_order_id: String,
                     out_trade_no: String,
                     trade_body: String,
                     var create_date: String = null,
                     var create_hour: String = null){
  //2020-07-17 03:18:55
  create_date=create_time.substring(0,10)
  create_hour=create_time.substring(11,13)

  //敏感信息脱敏--姓名
  consignee=consignee.substring(0,1)+"**"
  //手机号码
  consignee_tel=consignee_tel.replaceAll("(\\d{3})\\d{4}(\\d{4})","$1****$2")
}

```

> 在样例类中脱敏数据，对用户姓名、手机号码等信息进行脱敏

#### 数据结构转化

```scala
package com.orange.lin.app
import com.alibaba.fastjson.JSON
import com.orange.lin.Constant
import com.orange.lin.bean.OrderInfo
import com.orange.lin.util.MyKafkaUtil
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-15:56
 */
object  OrderApp extends BaseApp {
  override def run(ssc: StreamingContext): Unit = {
    //获取流
      val ds: DStream[String] = MyKafkaUtil.getKafkaStream(ssc,Constant.ORDER_INFO_TOPIC)
    //结构转化
      val orderInfoStream: DStream[OrderInfo] = ds.map(jsonStr=>JSON.parseObject(jsonStr,classOf[OrderInfo]))

  }
}

```

### 5.3 流的输出-HBase

```scala
package com.orange.lin.app
import com.alibaba.fastjson.JSON
import com.orange.lin.Constant
import com.orange.lin.bean.OrderInfo
import com.orange.lin.util.MyKafkaUtil
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-17-15:56
 */
object  OrderApp extends BaseApp {
  override def run(ssc: StreamingContext): Unit = {
    //获取流
      val ds: DStream[String] = MyKafkaUtil.getKafkaStream(ssc,Constant.ORDER_INFO_TOPIC)
    //结构转化
      val orderInfoStream: DStream[OrderInfo] = ds.map(jsonStr=>JSON.parseObject(jsonStr,classOf[OrderInfo]))

    //把数据写入HBase
    orderInfoStream.foreachRDD(rdd=>{
      import org.apache.phoenix.spark._
      rdd.saveToPhoenix("GMALL_ORDER_INFO",
        Seq("ID", "PROVINCE_ID", "CONSIGNEE", "ORDER_COMMENT", "CONSIGNEE_TEL", "ORDER_STATUS", "PAYMENT_WAY", "USER_ID", "IMG_URL", "TOTAL_AMOUNT", "EXPIRE_TIME", "DELIVERY_ADDRESS", "CREATE_TIME", "OPERATE_TIME", "TRACKING_NO", "PARENT_ORDER_ID", "OUT_TRADE_NO", "TRADE_BODY", "CREATE_DATE", "CREATE_HOUR"),
        zkUrl = Option("hadoop109,hadoop110,hadoop111:2181")
      )
    })
  }
}

```



## 6 数据查询接口

需求1：销售总额 sum(total_amount)

需求2：分时销售总额 ，（按小时分组后求取sum）

### 6.1 模块从属说明

该查询接口与实时数仓一同属与`gmall-publisher`，属于一个查询接口下的多个指标



### 6.2 Mapper数据层

#### 6.2.1 Mapper层接口类

```java
package com.orange.lin.gmallpublisher.mapper;

import java.util.List;
import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-17-16:29
 */
public interface OrderMapper {
    //当日的销售总额

    Double getTotalAmount(String date);
    //当日销售额的时分统计

    List<Map<String,Object>> getHourAmount(String date);
}

```

#### 6.2.2 数据层XML文件

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper SYSTEM "http://mybatis.org/dtd/mybatis-3-mapper.dtd" >
<!--namespace 对应前面定义的接口-->
<mapper namespace="com.orange.lin.gmallpublisher.mapper.OrderMapper">
    <!--对应前面接口中的方法,  标签内写响应的查询语句, 查询的接口会赋值给这个方法的返回值-->
    <select id="getTotalAmount" resultType="java.lang.Double">
        select sum(TOTAL_AMOUNT) from GMALL_ORDER_INFO where CREATE_DATE=#{date }
    </select>
    <select id="getHourAmount" resultMap="hourOrderList">
        select CREATE_HOUR,sum(TOTAL_AMOUNT) SUM from GMALL_ORDER_INFO where CREATE_DATE=#{date} group by CREATE_HOUR
    </select>
    <resultMap id="hourOrderList" type="java.util.Map"></resultMap>

</mapper>
```

### 6.3 Service服务层

#### 6.3.1 Service层接口

```java
package com.orange.lin.gmallpublisher.service;

import org.springframework.stereotype.Service;

import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:04
 */

public interface PublisherService {
    //获取总得日活
    Long getDau(String date);
       /*
        数据层
        // List(Map("loghour": "10", count: 100), Map,.....)
        List<Map<String, Object>> getHourDau(String date);

        //  Map("10"->100, "11"->200. "12"->100)
     */
    Map<String, Long> getHourDau(String date);
//==========================================

    //销售总额
    Double getTotalAmount(String date);

	//分时销售总额
    Map<String,Double>getHourAmount(String date);

}

```

#### 6.3.2 Service层实现类

```java
package com.orange.lin.gmallpublisher.service;

import com.orange.lin.gmallpublisher.mapper.DauMapper;
import com.orange.lin.gmallpublisher.mapper.OrderMapper;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.math.BigDecimal;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:05
 */
@Service
public class PublisherServiceImp implements PublisherService {

    @Autowired
    DauMapper dau;

    @Override
    public Long getDau(String date) {
        return dau.getDau(date);
    }
    /*

  数据层
  // List(Map("loghour": "10", count: 100), Map,.....)
  List<Map<String, Object>> getHourDau(String date);
  select LOGHOUR, count(*) COUNT from GMALL_DAU where LOGDATE=#{date } group by LOGHOUR


  //  Map("10"->100, "11"->200. "12"->100)

*/
    @Override
    public Map<String, Long> getHourDau(String date) {

        List<Map<String, Object>> hourDau = dau.getHourDau(date);

        Map<String, Long> result = new HashMap<>();

        for (Map<String, Object> map : hourDau) {
            String key = map.get("LOGHOUR").toString();
            Long value = (Long) map.get("COUNT");
            result.put(key, value);
        }
        return result;
    }
    //以上为日活指标
    //========================================
    @Autowired
    OrderMapper order;

    @Override
    public Double getTotalAmount(String date) {
        Double result = order.getTotalAmount(date);
        return result == null ? 0 : result;
    }

    @Override
    public Map<String, Double> getHourAmount(String date) {
        List<Map<String, Object>> hourAmount = order.getHourAmount(date);
        HashMap<String, Double> resultMap = new HashMap<>();
        for (Map<String, Object> map : hourAmount) {
            String key =(String) map.get("CREATE_HOUR");
            double value = ((BigDecimal) map.get("SUM")).doubleValue();
            resultMap.put(key, value);
        }
        return resultMap;
    }
}

```

### 6.4 controller控制层

> 补充显示销售总额部分以及分时统计部分

```java
package com.orange.lin.gmallpublisher.controller;

import com.alibaba.fastjson.JSON;
import com.orange.lin.gmallpublisher.service.PublisherService;
import it.unimi.dsi.fastutil.Hash;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import java.time.LocalDate;
import java.util.*;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:07
 */
@RestController
public class publisherController {
    @Autowired
    PublisherService service;

    @GetMapping("/realtime-total")
    public  String realtimeTotal(String date){

        //日活相关
        Long dau = service.getDau(date);
        // json字符串先用java的数据结构表示, 最后使用json序列化工具直接转成json字符串
        List<Map<String,String>> result = new ArrayList<>();

        HashMap<String, String> map1 = new HashMap<>();

        result.add(map1);
        map1.put("id", "dau");
        map1.put("name", "新增日活");
        map1.put("value", dau.toString());

        Map<String, String> map2 = new HashMap<>();
        result.add(map2);
        map2.put("id", "new_mid");
        map2.put("name", "新增设备");
        map2.put("value", "233");

        //销售总额
        //{"id":"order_amount","name":"新增交易额","value":1000.2 }
        HashMap<String, String> map3 = new HashMap<>();
        result.add(map3);
        map3.put("id", "order_amount");
        map3.put("name", "新增交易额");
        map3.put("value", service.getTotalAmount(date).toString());

        return JSON.toJSONString(result);
    }
    @GetMapping("/realtime-hour")
    public String getRealTimeHour(String id,String date){
        if ("dau".equals(id)){
            Map<String, Long> today = service.getHourDau(date);
            Map<String, Long> yesterday = service.getHourDau(getYesterday(date));
                        /*
            {"yesterday":{"11":383,"12":123,"17":88,"19":200 },
                "today":{"12":38,"13":1233,"17":123,"19":688 }}
             */
            HashMap<String , Map<String,Long>> result = new HashMap<>();
            result.put("today", today);
            result.put("yesterday", yesterday);

            return JSON.toJSONString(result);
        }else if ("order_amount".equals(id)){
            Map<String, Double> today = service.getHourAmount(date);
            Map<String, Double> yesterday = service.getHourAmount(date);

            HashMap<String , Map<String,Double>> result = new HashMap<>();
            result.put("today", today);
            result.put("yesterday", yesterday);
            return JSON.toJSONString(result);
        }
        else {
            return  null;
        }

    }
    /**
     * 返回昨天的年月日
     *
     * @param date
     * @return
     */
    private String getYesterday(String date) {
        return LocalDate.parse(date).plusDays(-1).toString();
    }


}

/*
1.	日活总数:
http://localhost:8070/realtime-total?date=2020-02-11

2.	日活分时统计
http://localhost:8070/realtime-hour?id=dau&date=2020-07-15

1.	日活总数
[{"id":"dau","name":"新增日活","value":1200},
{"id":"new_mid","name":"新增设备","value":233} ]


2.	日活分时统计
{"yesterday":{"11":383,"12":123,"17":88,"19":200 },
"today":{"12":38,"13":1233,"17":123,"19":688 }}



 */
```

## 7 数据可视化

无



## 8 数据完整启动流程

1. 启动HDFS、Zookeeper、Kafka、HBase
2. 启动publisher数据接口
3. 启动OrderApp流处理
4. 启动Canal客户端接收业务数据后作为Kafka集群生产者
5. 启动mysql客户端，选中相应数据库，使用存储过程

```sql
call init_data("2019-05-16", 10,2,false)
```

6. 启动数据可视化工具（可选）

> 重读第四步，观察数据变化
>
> ![1594991582828](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233152.png)
>
> 



## 补充 正则表达式

```
grep -e 使用扩展正则表达式

```

### 通配符

```shell
[atguigu@hadoop109 ~]$ touch a
[atguigu@hadoop109 ~]$ touch ab
[atguigu@hadoop109 ~]$ touch abc
[atguigu@hadoop109 ~]$ touch xyz
[atguigu@hadoop109 ~]$ ll
总用量 4
-rw-rw-r--  1 atguigu atguigu    0 7月  17 19:29 a
-rw-rw-r--  1 atguigu atguigu    0 7月  17 19:29 ab
-rw-rw-r--  1 atguigu atguigu    0 7月  17 19:29 abc
drwxrwxr-x. 2 atguigu atguigu 4096 7月  15 16:43 bin
-rw-rw-r--  1 atguigu atguigu    0 7月  17 19:29 xyz


#1.找出文件名以a开头的
[atguigu@hadoop109 ~]$ ls -l a*
-rw-rw-r-- 1 atguigu atguigu 0 7月  17 19:29 a
-rw-rw-r-- 1 atguigu atguigu 0 7月  17 19:29 ab
-rw-rw-r-- 1 atguigu atguigu 0 7月  17 19:29 abc
# * ：0-多个任意字符

#2.找出文件名以a开头的，文件名只有2个字符的
[atguigu@hadoop109 ~]$ ls -l ./a?
-rw-rw-r-- 1 atguigu atguigu 0 7月  17 19:29 ab
#3.找出文件名以a开头的，文件名只有3个字符的
[atguigu@hadoop109 ~]$ ls -l a??
-rw-rw-r-- 1 atguigu atguigu 0 7月  17 19:29 abc



```

### 匹配操作符



![1594985763967](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233204.png)

> **[字符匹配：**]()
>
> [1249a]：匹配到中括号内任意一个单个字符计算命中
>
> [^12] ：^表示取反，只要不包含1和2，就可以匹配上
>
> [a-k] ：给定一个序列，满足序列内的单个字符即可匹配
>
> 注意该匹配是单字符的匹配
>
> **位置匹配**：
>
> 行位置：
>
> `^x`:以x为首
>
> `x$`:以x结尾
>
> 单词边界
>
> `\<abc`：以abc结尾
>
> \：只为了转义

### 重复操作符

作用：不会操作数据，只会让前面匹配重复

![1594986519951](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233212.png)



正则表达式

分为基本正则表达式和扩展正则表达式

```
grep basic  
grep只能匹配基本正则表达式

如果grep需要使用扩展正则表达式
需要使用转义\或者 是-e 表示匹配扩展正则表达式

```



```
. : 匹配操作符，匹配任意单个字符
* ：重复操作符，重复0-多次

.* :表示匹配0-多次的任意个字符  ===通配符 *
```













​		

