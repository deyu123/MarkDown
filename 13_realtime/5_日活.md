# 实时数仓之日活

***

## 一、 需求概述

### 1.1 离线与实时需求

```sql
-- 1. 离线需求：t + 1 
       a、一般是根据前一日的数据生成报表等数据
       b、统计指标、报表繁多
       c、但是对时效性不敏感。
-- 2. 实时需求：t + 0
      a、侧重于对当日数据的实时监控
      b、通常业务逻辑相对离线需求简单
      c、统计指标也少一些
      e、更注重数据的时效性以及用户的交互性。
```

### 1.2 离线处理架构

离线分析架构（如Hive，Map/Reduce，Spark Sql等）可以满足数据后分析，数据挖掘的应用需求。

![1594884977794](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233301.png)



### 1.3 实时处理架构

对于实时性要求高的应用，如用户即时详单查询，业务量监控等，需要应用实时处理架构。

![1594884986844](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233307.png)

### 1.4 需求

```sql
本次一共有如下6个需求需要进行实现，那么本案例中只说明4个需求。
'需求1': 当日活跃用户及分时趋势图，昨日对比图(主讲)
'需求2': 当日新增用户数及分时趋势图，昨日对比图
'需求3': 当日交易额及分时趋势图，昨日对比图(主讲)
'需求4': 当日订单数及分时趋势图，昨日对比图
'需求5': 购物券功能风险预警
'需求6': 用户购买明细灵活分析功能

需求说明：
1. sparkStreaming实时消费kakfa中的数据；
2. mysql中的业务数据使用cannl实时监控，一旦出现变化的数据，就发送到kafka中。
3. sparkstreaming处理后的数据会可以发送到es、hbase、mysql、redis中
4. 前端通过可视化软件或者是通过springboot创建接口来实现
```

### 1.5 实现原理

```sql
-- 1. 框架：
   第一部分：模拟数据，代码生成
   第二部分：搭建nginx
   第三部分1：搭建日志服务器，并将数据写到kafka中，使用springboot实现
   第三部分2：搭建canal，实时监控mysql中数据的变化，并将变化的数据写到kafka中
   第四部分：编辑sparkstreaming代码，实现从kafka中读取数据，并进行数据的处理
   第五部分: 处理好的数据写到redis、mysql、es、hbase中。
   第六部分：可以mybaits创建接口来实现，也可以通过可视化界面来实现。
   
-- 2. 不同的需求采用不同的实现原理。
    '需求1': 当日活跃用户及分时趋势图，昨日对比图(主讲)
    	--流程：数据模拟 -> nginx -> 日志服务器 -> kafka -> sparkstreaming ->  hbase -> mybaits -> 网页展示（可视化）
    '需求2': 当日新增用户数及分时趋势图，昨日对比图 --不讲
    '需求3': 当日交易额及分时趋势图，昨日对比图
    	--流程：数据模拟 -> nginx -> 日志服务器 -> mysql -> kafka -> sparkstreaming ->  hbase -> mybaits -> 数据展示
    '需求4': 当日订单数及分时趋势图，昨日对比图  -- 不讲
    '需求5': 购物券功能风险预警
        --流程：数据模拟 -> nginx -> 日志服务器  -> kafka -> sparkstreaming ->  es-> kibana可视化
    '需求6': 用户购买明细灵活分析功能

-- 3. 本文梳理的过程为：
     1. 数据采集：服务器写到kafka中
        a、用户行为数据，从模拟数据，到接收到web服务器数据，再从日志服务器到kafka；
        b、业务数据采集，从数据库实时采集到kafka中
     2. 实时计算：redis ， hbase + phoenix，结果存储到es、mysql、redis、hbase
     3. 数据接口服务开发，为前
     4. Bi、可视化配置
     
-- 4. 每一个不同的需求都会有很多的依赖需要添加，那么添加一个依赖，咱们说一个。
```

## 二、项目准备部分

<img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233313.png" alt="1594885004104" style="zoom:150%;" />

> 本次实时项目省略flume

### 2.1 创建gmall0213工程

创建maven项目

```sql
-- 1. 该工程的有一个主要的任务：
   负责依赖和依赖版本的控制的作用。
-- 2. 依赖文件说明：
   1. properties中是定义了一些变量，后续使用时，只需要使用变量的值即可，当需求进行版本升级时，只需要变更变量的值就可以实现。
    <properties>
  .......
	</properties>
   2. 属于版本的声明，子模块继承父模块时，则不需要再声明版本，也是用于依赖版本的管理。
     <dependencyManagement>
      ....
      </dependencyManagement>
-- 3. 父工程的作用：依赖版本的管理。
```

- 添加依赖

```xml
<properties>
    <spark.version>2.4.5</spark.version>
    <scala.version>2.12.10</scala.version>
    <log4j.version>1.2.17</log4j.version>
    <slf4j.version>1.7.22</slf4j.version>

    <fastjson.version>1.2.47</fastjson.version>
    <httpclient.version>4.5.5</httpclient.version>
    <httpmime.version>4.3.6</httpmime.version>

    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
    <java.version>1.8</java.version>
</properties>

<dependencies>
    <!--此处放日志包，所有项目都要引用-->
    <!-- 所有子项目的日志框架 -->
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>jcl-over-slf4j</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-log4j12</artifactId>
        <version>${slf4j.version}</version>
    </dependency>
    <!-- 具体的日志实现 -->
    <dependency>
        <groupId>log4j</groupId>
        <artifactId>log4j</artifactId>
        <version>${log4j.version}</version>
    </dependency>
</dependencies>

<dependencyManagement>
    <dependencies>
        <!-- https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient -->
        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpclient</artifactId>
            <version>${httpclient.version}</version>
        </dependency>
        <dependency>
            <groupId>org.apache.httpcomponents</groupId>
            <artifactId>httpmime</artifactId>
            <version>${httpmime.version}</version>
        </dependency>

        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>${fastjson.version}</version>
        </dependency>


        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.11</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>${spark.version}</version>
        </dependency>
    </dependencies>
</dependencyManagement>
<build>
    <plugins>
        <!-- 该插件用于将Scala代码编译成class文件 -->
        <plugin>
            <groupId>net.alchim31.maven</groupId>
            <artifactId>scala-maven-plugin</artifactId>
            <version>3.4.6</version>
            <executions>
                <execution>
                    <!-- 声明绑定到maven的compile阶段 -->
                    <goals>
                        <goal>compile</goal>
                        <goal>testCompile</goal>
                    </goals>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```



- **Maven整体依赖结构**

![1594812372205](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233329.png)



### 2.2 创建common子模块

```sql
-- 1. 该模块的作用：这个模块内放一些通用代码，各大模块都会使用到的代码
-- 2. 所有模块都会加入common模块的依赖
```

1. 模块名：gmall-common

2. 添加依赖

```xml
<dependencies>
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
    </dependency>

    <dependency>
        <groupId>org.apache.httpcomponents</groupId>
        <artifactId>httpclient</artifactId>
    </dependency>
    <dependency>
        <groupId>org.apache.httpcomponents</groupId>
        <artifactId>httpmime</artifactId>
    </dependency>
</dependencies>
```

## 三、模拟数据部分

```sql
-- 1. 说明：
   该模拟数据部分，在实际开发过程中，是实时的数据，不需要我们处理，但是可以用来做一些数据验证。
-- 2. 代码：
   此部分的代码不要求掌握，需要知道含义和逻辑就可以。
-- 3. 模拟数据发送流程：
   一共分为三个部分：
   1. 模拟数据发送日志请求  --> 由Windows idea springboot web 服务器接收并发送到kafka，落盘日志文件；
   2. springboot 打包 -> 部署到linux下 -> 模拟数据发送日志请求 -> jar程序接收，发送kafka，并落盘数据
   3. 模拟数据发送日志请求 -> nginx -> 三台日志服务器 -> 三台一起写入kafka，落盘日志文件。
-- 4. 该部分属于发送数据，由于不同模式下参数有所差异，届时一一说明
```

1. 模块名：gmall-mock
2. 添加依赖

```xml
    <dependencies>
        <dependency>
            <artifactId>gmall0213</artifactId>
            <groupId>com.atguigu</groupId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
    </dependencies>
```

### 3.1 RandomNumUtil 工具类

```sql
-- 1. 该类的作用：生成随机整数的工具类  
```

- 代码

```scala

import scala.collection.mutable
import scala.util.Random

/**
  * 生成随机数据的工具
  */
object RandomNumUtil {
  /*随机数生成器对象*/
  private val random = new Random()

  /**
    * 生成随机的整数, 区间: [from, to]
    *
    * @param from
    * @param to
    * @return
    */
  def randomInt(from: Int, to: Int): Int = {
    if (from > to) throw new IllegalArgumentException(s"from: $from 不能大于 to: $to")
    else random.nextInt(to - from + 1) + from
  }

  /**
    * 创建多个 Int 值
    *
    * @param from
    * @param to
    * @param count     创建的 Int 值的顺序
    * @param canRepeat 是否允许重复
    * @return List[Int] 集合
    */
  def randomMultiInt(from: Int, to: Int, count: Int, canRepeat: Boolean = true): List[Int] = {
    if (canRepeat) {
      (1 to count).toList.map(_ => randomInt(from, to))
    } else {
      val set = mutable.Set[Int]()
      while (set.size < count) {
        set += randomInt(from, to)
      }
      set.toList
    }
  }

  /**
    * 生成一个随机的 Long 值 范围: [from, to]
    *
    * @param from
    * @param to
    * @return
    */
  def randomLong(from: Long, to: Long): Long = {
    if (from > to) throw new IllegalArgumentException(s"from: $from 不能大于 to: $to")
    else math.abs(random.nextLong) % (to - from + 1) + from
  }
}

```

### 3.2 RadomOptions工具类

```sql
-- 作用：按照一定的分布生成随机选项的工具类
```

- 代码

```scala
package com.atguigu.gmall.mock.util

/**
  * @Description
  **
  * @author lianzhipeng
  * @create 2020-07-21 2:13:32
  */
import scala.collection.mutable.ListBuffer

/**
  * 根据提供的值和比重, 来创建RandomOptions对象.
  * 然后可以通过getRandomOption来获取一个随机的预定义的值
  */
object RandomOptions {
  def apply[T](opts: (T, Int)*) = {
    val randomOptions = new RandomOptions[T]()
    randomOptions.totalWeight = opts.foldLeft(0) (_ + _._2) // 计算出来总的比重
    opts.foreach {
      case (value, weight) => randomOptions.options ++= (1 to weight).map(_ => value)
    }
    randomOptions
  }
}

class RandomOptions[T] {
  var totalWeight: Int = _
  var options = ListBuffer[T]()

  /**
    * 获取随机的 Option 的值
    *
    * @return
    */
  def getRandomOption() = {
    options(RandomNumUtil.randomInt(0, totalWeight - 1))
  }
}

```

> Scala会自动识别apply方法，所以调用伴生对象的apply方法时，apply方法名可以省略，这里的`RandomOptions`构建对象就是用apply的方式
>

### 3.3 LogUploader 日志发送工具类

```sql
作用：向服务器发送生成的日志的工具类
```

- 代码

```scala
package com.atguigu.gmall.mock.util

import java.io.OutputStream
import java.net.{HttpURLConnection, URL}

/**
  * Author lzc
  * Date 2019/5/14 11:25 AM
  */
object LogUploader {
  /*发送日志*/
  def sendLog(log: String): Unit = {
    try {
      // 1. 日志服务器的地址
      val logUrl = new URL("http://localhost:8080/log")
      // 2. 得到一个 HttpURLConnection
      val conn: HttpURLConnection = logUrl.openConnection().asInstanceOf[HttpURLConnection]
      // 3. 设置请求方法(上传数据一般使用 post 请求)
      conn.setRequestMethod("POST")
      // 4. 用来供server进行时钟校对的
      conn.setRequestProperty("clientTime", System.currentTimeMillis + "")
      // 5. 允许上传数据
      conn.setDoOutput(true)
      // 6. 设置请求的头信息, post 请求必须这样设置
      conn.setRequestProperty("Content-Type", "application/x-www-form-urlencoded")
      // 7. 获取上传用的输出流
      val out: OutputStream = conn.getOutputStream
      // 8. 写出数据
      out.write(("log=" + log).getBytes())
      // 9. flush
      out.flush()
      // 10. 关闭资源
      out.close()
      // 11. 获取响应码.  (或者获取响应信息也行, 否则不会发送请求到服务器)
      val code: Int = conn.getResponseCode
      println(code)
    } catch {
      case e: Exception => e.printStackTrace()
    }
  }
}
```

> **Q:请求方法中post和get的区别与作用**
>
> ![1594730664181](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233345.png)
>
> 
>
> **Q:post请求文本信息的说明**
>
> 服务器端通常是根据请求头（headers）中的` Content-Type `字段来获知请求中的消息主体是用**何种方式编码**，再对主体进行解析。所以说到 POST 提交数据方案，包含了 Content-Type 和消息主体编码方式两部分
>
> ```scala
>  //文本信息上传的设置
> conn.setRequestProperty("Content-Type", "application/x-www-form-urlencoded")
> 
> //提交的的数据,请求body中按照 key1=value1&key2=value2 进行编码,key和value都要进行urlEncode;
> ```
>
> Q:响应码的说明
>
> 1XX：临时相应
>
> 2XX：成功相应
>
> 3XX：重定向，（访问的资源路径可能发生改变）
>
> 4XX：客户端错误
>
> ​	400：请求的语法错误
>
> ​	401：没有身份验证，一般账号密码正确可以通过
>
> ​	403：没有权利访问，被服务器拒绝
>
> ​	404：服务器找不到资源，最常见
>
> 5XX：服务器端语法错误
>
> ​	500：服务器未知错误
>
> ​	503：一般是服务器维护，当前无法处理请求
>
> 

### 3.4 JsonMock 生成日志

```sql
-- 1. 作用：生成模拟数据
-- 2. 数据内容说明：
     1. json对象
     2. 数据没有写日志生成时间，待收到数据时添加时间戳
     3. 有启动日志和事件日志
```

- 代码

```scala
package com.atguigu.gmall.mock
import java.util.Date

import com.alibaba.fastjson.{JSON, JSONObject}
import com.atguigu.gmall.mock.util.{LogUploader, RandomNumUtil, RandomOptions}


object JsonMock {

  val startupNum = 100000 // 生成的启动日志的记录数
  val eventNum = 200000 // 生成的事件日志的记录数

  // 操作系统的分布
  val osOpts = RandomOptions(("ios", 3), ("android", 7))

  // 日志开始时间
  var startDate: Date = _
  // 日志结束时间
  var endDate: Date = _

  // 地理位置分布
  val areaOpts = RandomOptions(
    ("beijing", 20), ("shanghai", 20), ("guangdong", 20),
    ("hebei", 5), ("heilongjiang", 5), ("shandong", 5),
    ("tianjin", 5), ("guizhou", 5), ("shangxi", 5),
    ("sichuan", 5), ("xinjiang", 5)
  )

  // appId
  val appId = "gmall"

  // app 的版本分布
  val versionOpts = RandomOptions(
    ("1.2.0", 50), ("1.1.2", 15),
    ("1.1.3", 30), ("1.1.1", 5))

  // 用户行为的分布(事件分布)
  val eventOpts = RandomOptions(
    ("addFavor", 10), ("addComment", 30),
    ("addCart", 20), ("clickItem", 40))

  // app 分发渠道分布
  val channelOpts = RandomOptions(
    ("xiaomi", 10), ("huawei", 20), ("wandoujia", 30),
    ("360", 20), ("tencent", 20), ("baidu", 10), ("website", 10))

  // 生成模拟数据的时候是否结束退出
  val quitOpts = RandomOptions((true, 5), (false, 95))

  // 模拟出来一条启动日志
  def initOneStartupLog(): String = {
    /*
    `logType` string   COMMENT '日志类型',
    `mid` string COMMENT '设备唯一标识',
    `uid` string COMMENT '用户标识',
    `os` string COMMENT '操作系统', ,
    `appId` string COMMENT '应用id', ,
    `version` string COMMENT '版本号',
    `ts` bigint COMMENT '启动时间',    考虑每个终端的时间的不准群性, 时间是将来在服务器端来生成
    `area` string COMMENT '城市'
    `channel` string COMMENT '渠道'
     */
    val mid: String = "mid_" + RandomNumUtil.randomInt(1, 500)
    val uid: String = "" + RandomNumUtil.randomInt(1, 10000)
    val os: String = osOpts.getRandomOption()
    val appId: String = this.appId
    val area: String = areaOpts.getRandomOption()
    val version: String = versionOpts.getRandomOption()
    val channel: String = channelOpts.getRandomOption()

    val obj = new JSONObject()
    obj.put("logType", "startup")
    obj.put("mid", mid)
    obj.put("uid", uid)
    obj.put("os", os)
    obj.put("appId", appId)
    obj.put("area", area)
    obj.put("channel", channel)
    obj.put("version", version)
    // 返回 json 格式字符串
    obj.toJSONString
  }

  // 模拟出来一条事件日志  参数: json 格式的启动日志
  def initOneEventLog(startupLogJson: String) = {
    /*`
    logType` string   COMMENT '日志类型',
    `mid` string COMMENT '设备唯一标识',
    `uid` string COMMENT '用户标识',
    `os` string COMMENT '操作系统',
    `appId` string COMMENT '应用id',
    `area` string COMMENT '地区' ,
    `eventId` string COMMENT '事件id',
    `pageId` string COMMENT '当前页',
    `nextPageId` string COMMENT '跳转页',
    `itemId` string COMMENT '商品编号',
    `ts` bigint COMMENT '时间'
     */
    val startupLogObj: JSONObject = JSON.parseObject(startupLogJson)

    val eventLogObj = new JSONObject()
    eventLogObj.put("logType", "event")
    eventLogObj.put("mid", startupLogObj.getString("mid"))
    eventLogObj.put("uid", startupLogObj.getString("uid"))
    eventLogObj.put("os", startupLogObj.getString("os"))
    eventLogObj.put("appId", this.appId)
    eventLogObj.put("area", startupLogObj.getString("area"))
    eventLogObj.put("eventId", eventOpts.getRandomOption())
    eventLogObj.put("pageId", RandomNumUtil.randomInt(1, 50))
    eventLogObj.put("nextPageId", RandomNumUtil.randomInt(1, 50))
    eventLogObj.put("itemId", RandomNumUtil.randomInt(1, 50))
    eventLogObj.toJSONString
  }

  // 开始生成日志
  def generateLog(): Unit = {
    (0 to startupNum).foreach(_ => {
      // 生成一条启动日志
      val oneStartupLog: String = initOneStartupLog()
      // 发送启动日志
      LogUploader.sendLog(oneStartupLog)
      // 模拟出来多条事件日志
      while (!quitOpts.getRandomOption()) {
        // 生成一条事件日志
        val oneEventLog: String = initOneEventLog(oneStartupLog)
        // 发送事件日志
        LogUploader.sendLog(oneEventLog)
        Thread.sleep(100)
      }
      Thread.sleep(1000)
    })
  }

  def main(args: Array[String]): Unit = {
    // 测试
    generateLog()
  }
}

```

## 四、数据采集部分

```sql
-- 1. 说明：
   日志服务器我们使用springboot来实现，待流程疏通以后，我们再使用nginx来实现负载均衡。
```

- 流程

![1594730753932](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233358.png)



### 4.1 spring boot简介

```sql
-- 1. Spring boot 好处
    1. 内嵌 Tomcat, 不再需要外部的 Tomcat
    2. 不再需要那些千篇一律，繁琐的 xml 文件。
    3. 更方便的和各个第三方工具（ mysql,redis,elasticsearch,dubbo 等等整合），而只要维护一个配置文件即可
-- 2. 配置文件说明
	1.springboot 实际上就是把以前需要用户手工配置的部分，全部作为默认项。除非用户需要额外更改不然不用配置。
	 这就是所谓的: '约定大于配置'
	2.如果需要特别配置的时候，去修改'application.properties'
```

### 4.2 spring boot搭建

#### 4.2.1 创建子模块 gmall-logger

作用：日志采集服务器的创建

![1594730896851](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233418.png)

<img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200720193239.png" alt="image-20200720193025377" style="zoom: 67%;" />

<img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200720193432.png" alt="image-20200720193432593" style="zoom:50%;" />

![1594730921222](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233432.png)

> 收到的消息要发往kafka集群，需要配置kafka依赖

#### 4.2.2 模块依赖说明

1. 修改依赖关系

```sql
-- 1. 依赖的操作：
   1. 把该子项目依赖的父工程来替换成我们自己的父工程
   2. 在我们的父工程中添加子项目
```

-  把该子项目依赖的父工程来替换成我们自己的父工程

```xml
  <parent>
        <groupId>com.atguigu</groupId>
        <artifactId>gmall0213</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>
```

- 在我们的父工程中添加子项目，并将上面创建时的父依赖作为父工程的父依赖

```xml
  <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>2.3.1.RELEASE</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
<modules>
        <module>gmall-common</module>
        <module>gmall-mock</module>
        <module>gmall-logger</module>
    </modules>

```

2. logger模块添加gmall-commo依赖

```xml
		 <dependency>
            <groupId>com.atguigu</groupId>
            <artifactId>gmall-common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
```

### 4.3 Controller的创建

```sql
-- 1. 服务器的需求：
       1. 给日志添加时间
       2. 日志数据落盘，为离线需求做准备
       3. 日志发往kafka中。
-- 2. 在gmall-logger创建一个类：LoggerController,用来处理http请求
```

1. 测试是否能接收到并处理网页的请求

- 创建LoggerController类，并加上注解如下3个注解。

```java
@ResponseBody
@Controller
public class LoggerController {
   @GetMapping("/log")
    public String doLog(){
        return "ok";
    }
}
```

- 在网页：localhost:8080/log访问时，会返回”ok“

![image-20200722015707810](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722015715.png)

2. 测试通过模拟数据，是否能接收到数据

- 模拟数据发送的地址为："http://localhost:8080/log"

![image-20200722015834774](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722015834.png)

- LoggerController代码

```java
@ResponseBody
@Controller
public class LoggerController {
   @PostMapping("/log")  //将请求改成postMapping
    public String doLog(@RequestParam("log") String log){ //加上参数注解，如果参数的名称和请求参数一致，可以省略这个注解
       System.out.println(log); // 打印请求的数据
        return "ok";
    }
}
```

- 打开模拟数据，再打开服务器。能收到请求的数据

![image-20200722021843525](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722021843.png)

#### 4.3.1 日志落盘

- log4j实现日志落盘

添加log4j.properties 文件

```properties
log4j.appender.atguigu.MyConsole=org.apache.log4j.ConsoleAppender
log4j.appender.atguigu.MyConsole.target=System.err
log4j.appender.atguigu.MyConsole.layout=org.apache.log4j.PatternLayout
log4j.appender.atguigu.MyConsole.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %6p (%c:%M) - %m%n 

log4j.appender.atguigu.File=org.apache.log4j.DailyRollingFileAppender
log4j.appender.atguigu.File.file=/opt/module/gmall/gmall.log
log4j.appender.atguigu.File.DatePattern='.'yyyy-MM-dd
log4j.appender.atguigu.File.layout=org.apache.log4j.PatternLayout
log4j.appender.atguigu.File.layout.ConversionPattern=%m%n

log4j.logger.com.orange.lin.gmalllogger.LoggerController=info,atguigu.File,atguigu.MyConsole
```

- 排除spring-boot默认支持的logging

```xml
<!-- 去除 logging-->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-web</artifactId>
    <exclusions>
        <exclusion>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-logging</artifactId>
        </exclusion>
    </exclusions>
</dependency>
<!-- 添加 spring log4j 支持 -->
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-log4j</artifactId>
    <version>1.3.8.RELEASE</version>
</dependency>

```

#### 4.3.2 发送到kafka

- 在文件Application.properties中添加kafka配置

```properties
server.port=8081
#kafka配置
spring.kafka.bootstrap-servers=hadoop109:9092,hadoop110:9092,hadoop111:9092
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
```

> 该配置指定了kafka集群的地址和端口，key和value的序列化
>

- 在 gmall-common子模块创建存放topic主题名字的类

```java
package com.orange.lin;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-14-10:06
 */
public class Constant {
    public final static String STARTUP_TOPIC ="startup_topic";
    public final static String EVENT_TOPIC="event_topic";
}

```

#### 4.3.3 业务实现

```java
package com.orange.lin.gmalllogger;

import com.alibaba.fastjson.JSON;
import com.alibaba.fastjson.JSONObject;
import com.orange.lin.Constant;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration;
import org.springframework.boot.jackson.JsonObjectDeserializer;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.PostMapping;
import org.springframework.web.bind.annotation.RestController;

import javax.sound.midi.Soundbank;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-13-20:01
 */
//    @RequestMapping(value = "/log", method = RequestMethod.POST)
//    @ResponseBody  //表示返回值是一个 字符串, 而不是 页面名

@RestController
public class LoggerController {
	//表示post上传web服务器/log时，执行的动作
    @PostMapping("/log")
    public String doLog(String log){
        //1. 给日志添加一个时间戳
        log=addTS(log);
        // 2. 数据落盘(为离线数据做准备)
        saveToDisk(log);
        // 3. 把数据写入到kafka, 需要写入到topic
        sendToKafka(log);
        return "ok";
    }

//可以对类成员变量、方法及构造函数进行标注，让 spring 完成 bean 自动装配的工作。
//@Autowired 默认是按照类去匹配，配合 @Qualifier 指定按照名称去装配 bean
    @Autowired
    KafkaTemplate kafka;

    /**
     * 日志发往kafka
     * 不同的日志写到不同的topic
     * @param log
     */
    private void sendToKafka(String log) {
        //为了避免除了type字段外的字符串出现相同字符串，加上""判定
        if (log.contains("\"startup\"")){
            kafka.send(Constant.STARTUP_TOPIC,log);
        }else {
            kafka.send(Constant.EVENT_TOPIC,log);
        }

    }
    Logger logger = LoggerFactory.getLogger(LoggerController.class);
    private void saveToDisk(String log) {
        logger.info(log);
    }

    /**
     * 添加时间戳
     * @param log
     * @return
     */
    public String addTS(String log){
        JSONObject jsonObject = JSON.parseObject(log);
        jsonObject.put("ts", System.currentTimeMillis());
        return JSON.toJSONString(jsonObject);
    }

}

```



### 4.4 Linux部署

目的：把数据采集部署到Linux 服务器

- 修改日志存放目录为 linux下的目录
- 打包项目，将日志服务器的jar包上传
- 启动日志采集服务器

> //方式一
> java -jar xxxx.jar  
> //jar中必须配置了main-class
>
> //方式二
> java -cp gmall-logger-0.0.1-SNAPSHOT.jar org.springframework.boot.loader.JarLauncher
>
> 两种方式的说明：
>
> java -jar命令执行时会运用到目录`META-INF\MANIFEST.MF`文件，在该文件中，有一个叫`Main－Class`的参数，它说明了java -jar命令执行的类。
>
> ![1594732961560](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233507.png)
>
> java -cp 和 -classpath 一样，是指定类运行所依赖其他类的路径，通常是类库，jar包之类，需要全路径到jar包，window上分号“;” 
>
> *分隔，linux上是分号“:”分隔。不支持通配符，需要列出所有jar包，用一点“.”代表当前路径。*
>
> 

## 五、 负载均衡Nginx的使用

> 该部分从属于数据采集部分，主要作用为实现三台数采服务器的负载均衡，因涉及新的技术---Nginx，所以单独成为一个part

![1594748402220](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233514.png)

### 5.1 Nginx概述

 Nginx (读作“engine x”), 是一个高性能的 HTTP 和反向代理服务器 , 特点是占有内存少，并发能力强

- 与tomcat的关系

除了 tomcat 以外， apache,nginx,jboss,jetty 等都是 http 服务器。

nginx 和 apache 只支持静态页面和 CGI 协议的动态语言，比如 perl 、 php 等， 但是nginx不支持 java 。

Java 程序只能通过与 tomcat 配合完成。   nginx 与 tomcat 配合，为 tomcat 集群提供反向代理服务、负载均衡等服务

#### 5.1.1 Nginx三大功能

##### 反向代理

- 正向代理

![1594748623769](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233520.png)

> 服务器代理用户的请求，从用户的角度看，没法直接获取请求，需要通过代理
> 特点：代理用户，用户清楚知道访问哪台服务器

- 反向代理

![1594748632948](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233526.png)

> 反向代理：服务器代理真正服务器，用户不确定去哪台真实服务器，

##### 负载均衡

•轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，则自动剔除故障机器，使用户访问不受影响

•weight 指定轮询权重，weight值越大，分配到的几率就越高，主要用于后端每台服务器性能不均衡的情况。

• 备机模式 平时不工作, 只有其他down 机的时候才会开始工作

• 公平模式(第三方) 更智能的一个负载均衡算法，此算法可以根据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。如果想要使用此调度算法，需要Nginx的upstream_fair模块。

##### 动静分离

![1594748722077](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233531.png)

### 5.2 Nginx安装

- yum安装依赖包

```shell
sudo yum -y install    openssl openssl-devel pcre pcre-devel    zlib zlib-devel gcc gcc-c++
```

- 下载Nginx

```shell
/opt/software » wget http://nginx.org/download/nginx-1.12.2.tar.gz
```

- 解压

```shell
tar -zxvf nginx-1.12.2.tar.gz -C /opt/module
```

- 编译和安装

进入解压缩的目录

为了防止出现权限问题, 建议切换到 root 用户

```shell
./configure   --prefix=/usr/local/webserver/nginx
make && make install

```

- **启停Nginx**

进入目录: /usr/local/webserver/nginx

```shell
启动 nginx: sbin/nginx
关闭 nginx: sbin/nginx -s stop
重新加载: sbin/nginx -s reload

```

> 注意：
>
> - Nginx 默认使用的是 80 端口, 由于非root用户不能使用 1024 以内的端口, 所以建议使用 root 用户启动 Nginx
> - 如果使用普通用户启动 Nginx, 需要先执行下面的命令来突破上面的限制:
>
> ```
> sudo setcap cap_net_bind_service=+eip /usr/local/webserver/nginx
> ```

- 查看Nginx进程

![1594748930149](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233543.png)

通过网页访问: http://hadoop109

![1594748945767](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233548.png)

### 5.3 配置负载均衡

#### 5.3.1 Nginx配置修改

-  修改/usr/local/webserver/nginx/conf/nginx.conf

```properties
http {
    .....

    # 配置上游服务器: 其实就被代理的服务器, springboot
    upstream logserver{
        server hadoop109:8081 weight=1;
        server hadoop110:8081 weight=1;
        server hadoop111:8081 weight=1;
    }
    server {
        listen       80;
        server_name  logserver;

        location / {
            root   html;
            index  index.html index.htm;
            # 配置代理
            proxy_pass http://logserver;
            proxy_connect_timeout 10;
        }

        ...
    }
}

```

> **Q：为什么不配置日志服务器端口为8080**
>
> 在kafka启动消费数据前要先打开zookeeper集群，在zookeeper3.5.0之后的版本中，集群打开后会默认占用8080端口
>
> ```shell
> [atguigu@hadoop109 module]$ jps
> 13317 Jps
> 13229 QuorumPeerMain
> [atguigu@hadoop109 module]$ netstat -tunlp |grep 8080
> (Not all processes could be identified, non-owned process info
>  will not be shown, you would have to be root to see it all.)
> tcp6       0      0 :::8080                 :::*                    LISTEN      13229/java 
> ```
>
> 通过观察日志可以发现确实启动了一个叫adminServer的服务
>
> ![1594749317598](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233556.png)
>
> 这是一个内嵌的jetty服务。如果想在zookeeper上解决这个问题，有以下三种方法
>
> ```
> （1）.删除jetty。
> （2）修改端口。
> 修改方法的方法有两种，一种是在启动脚本中增加 -Dzookeeper.admin.serverPort=你的端口号.一种是在zoo.cfg中增加admin.serverPort=没有被占用的端口号
> （3）停用这个服务，在启动脚本中增加”-Dzookeeper.admin.enableServer=false”
> ```
>
> ```shell
> netstat -tunlp|grep 端口号 查看占用端口的进程
> ```
>
> 

#### 5.3.2 日志采集服务器群起脚本制作

分别在 3 个节点启动 jar 比较麻烦, 制作统一启动脚本.

```shell
#!/bin/bash
case $1 in
    "start")
    {
        for i in hadoop201 hadoop202 hadoop203
        do
            echo "========启动日志服务: $i==============="
            ssh $i  "source /etc/profile ; java -jar /opt/module/gmall/gmall-logger-1.0-SNAPSHOT.jar >/dev/null 2>&1  &"
        done
     };;
    "stop")
    {
        for i in hadoop201 hadoop202 hadoop203
        do
            echo "========关闭日志服务: $i==============="
            ssh $i "ps -ef|grep gmall-logger-1.0-SNAPSHOT.jar | grep -v grep|awk '{print \$2}'|xargs kill" >/dev/null 2>&1
        done
    };;
    
    *)
    {
        echo 启动姿势不对, 请使用参数 start 启动日志服务, 使用参数 stop 停止服务
    };;
esac

```

#### 5.3.3 其他操作

- 分发启动采集服务器jar包到其余设备
- 给编写好的脚本增加执行权限
- 启动Nginx
- 启动脚本

> 脚本编写的注意事项：
>
> 如果在windows环境下编写脚本后复制到linux系统中出现如下报错：
>
> ```
> bad interpreter:
> /bin/bash^M: no such file or directory
> ```
>
> 是因为在 window 下写的脚本回车的时候使用的是\r\n, 而在 linux 使用\n就可以了, 所在每行的末尾多了一个\r.
>
> 使用下面的命令去掉行尾的\r:
>
> ```
> sed -i -e 's/\r$//' gmall_cluster
> ```

- 测试日志生成能否在集群中生成落盘日志与控制台打印

------



## 六、实时处理部分-- SparkStreaming（重要）

![1594749834377](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233605.png)

### 6.1 日活指标分析

```sql
-- 1. 什么叫日活
    1. 通常: 打开应用的用户即为活跃用户，不考虑用户的使用情况。每天一台设备打开多次会被计为一个活跃用户。
    2. 游戏用户: 每天打开/登录游戏的用户数（针对游戏DAU的定义）
-- 2. 需求：当日活跃用户及分时趋势图，昨日对比图
-- 3. 思路： 
      1. 使用sparkSteaming动态读取kafka中的数据
      2. 获取数据以后，对数据进行处理：
         a、
```

什么叫日活:

1. 


我们采用第一种日活的定义, 日活(DAU)统计思路:

1. 从 kafka 读取用户启动日志

2. 当天只保留用户的第一次启动记录, 过滤掉其他启动记录: 借助于 Redis

3. 然后把第一次启动记录保存在 hbase 以供其他应用查询

**整体思路：**

![1594750032810](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233611.png)

### 6.2 实时处理模块的创建

作用：实现具体需求

模块名：mall-realtime

#### 依赖导入

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>realtime-gmall</artifactId>
        <groupId>com.orange.lin</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.orange.lin</groupId>
    <artifactId>gmall-realtime</artifactId>
    <version>1.0-SNAPSHOT</version>
    <dependencies>
        <dependency>
            <groupId>com.orange.lin</groupId>
            <artifactId>gmall-common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
        </dependency>

        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>2.9.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-spark</artifactId>
            <version>5.0.0-HBase-2.0</version>
        </dependency>
    </dependencies>
    <build>
        <plugins>
            <!-- 用于项目的打包插件 -->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>


</project>
```

### 6.3 从Kafka中消费数据

#### 6.3.1 配置

```properties
kafka.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
group.id=bigdata
```

> kafka的配置转移到配置文件，不采用硬编码
>
> `kafka.servers`配置该项时，赋值给`bootstrap.servers`，用以指明kafka集群的服务器地址，此外还有一种旧版的写法如下：
>
> ```properties
> kafka.broker.list=hadoop102:9092,hadoop103:9092,hadoop104:9092
> ```
>
> 0.8以前的kafka，消费的进度(offset)是写在zk中的，所以consumer需要知道zk的地址。这个方案有性能问题，0.9 的时候整体大改了一次，brokers 接管了消费进度，consumer 不再需要和 zookeeper 通信了，所以就用bootstrap-server了。
>
> ![1594815191018](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233620.png)

#### 5.3.2 消费Kafka工具类的编写

##### 读取配置

```scala
package com.orange.lin.util

import java.io.InputStream
import java.util.Properties

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-15-8:50
 */
object ConfigUtil {
  private val is: InputStream = ClassLoader.getSystemResourceAsStream("config.properties")
  private val properties = new Properties()
  properties.load(is)


  def getProperty(fileName:String,name:String)={
    properties.getProperty(name)
  }

  def main(args: Array[String]): Unit = {
    println(getProperty("config.properties","bootstrap.servers"))
  }

}

```

##### 消费kafka数据

采用Direct方式消费kafka数据，目前receiver方式已经过时

```scala
package com.orange.lin.util

import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.kafka010.KafkaUtils
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-14-19:20
 */
object MyKafkaUtil {
  val kafkaParams:Map[String,Object]=Map[String,Object](
    "bootstrap.servers"->ConfigUtil.getProperty("config.properties","bootstrap.servers"),
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id"->ConfigUtil.getProperty("config.properties","group.id"),
    "auto.offset.reset"->"latest",
    "enable.auto.commit"->(true:java.lang.Boolean)
  )



  def getKafkaStream(ssc:StreamingContext,topic:String) ={
    KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent, // 标配
      /*
      LocationStrategies
        preferConsistent:平均分配，每个分区的数据量平均
        PreferBroker:如果kafka和executor都在同一台设备，使用 PreferBroker，（不需要网络传输）
        PreferFixed：有数据倾斜时需要选这个
       */
      Subscribe[String, String](Set(topic), kafkaParams)
    ).map(_.value())
  }
}

```

> **Q：receiver和Direct两种消费kafka数据的区别**
>
> Receiver接收方式：
>
> 方法：`KafkaUtils.createDstream`
>
> Receiver作为常驻的Task运行在Executor等待数据，但是一个Receiver效率低，需要开启多个，再手动合并数据(union)，再进行处理，很麻烦
> Receiver哪台机器挂了，可能会丢失数据，所以需要开启WAL(预写日志)保证数据安全，那么效率又会降低!
> Receiver方式是通过zookeeper来连接kafka队列，调用Kafka高阶API，offset存储在zookeeper，由Receiver维护。
> spark在消费的时候为了保证数据不丢也会在Checkpoint中存一份offset，可能会出现数据不一致
> 所以不管从何种角度来说，Receiver模式都不适合在开发中使用了,已经淘汰了
>
> Direct直连方式：
>
> 方法：`KafkaUtils.createDirectStream`
>
> Direct方式是直接连接kafka分区来获取数据，从每个分区直接读取数据大大提高了并行能力
> Direct方式调用Kafka低阶API(底层API)，offset自己存储和维护，默认由Spark维护在checkpoint中，消除了与zk不一致的情况
> 当然也可以自己手动维护，把offset存在mysql、redis中
> 所以基于Direct模式可以在开发中使用，且借助Direct模式的特点+手动操作可以保证数据的Exactly once 精准一次
>
> **总结：**
>
> **Receiver接收方式**
> 	多个Receiver接受数据效率高，但有丢失数据的风险
> 	开启日志（WAL）可防止数据丢失，但写两遍数据效率低。
> 	Zookeeper维护offset有重复消费数据可能。
> 	使用高层次的API
> **Direct直连方式**
> 	不使用Receiver，直接到kafka分区中读取数据
> 	不使用日志（WAL）机制
> 	Spark自己维护offset
> 	使用低层次的API

### 6.4 流的获取与数据结构调整

目的：从kafka消费得到的日志数据为json字符串，需要转换为结构化的json对象

#### 6.4.1 样例类生成

```scala
package com.orange.lin.bean

import java.text.SimpleDateFormat
import java.util.Date

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-15-11:24
 */
case class StartupLog(mid: String,
                      uid: String,
                      appId: String,
                      area: String,
                      os: String,
                      channel: String,
                      logType: String,
                      version: String,
                      ts: Long,
                      var logDate: String = null,   // 年月日  2020-07-15
                      var logHour: String = null)   //小时  10
{
  private val date = new Date(ts)
  logDate = new SimpleDateFormat("yyyy-MM-dd").format(date)
  logHour = new SimpleDateFormat("HH").format(date)

}

```

> Q: 为什么logDate和logHour声明为var，且先赋值为null
>
> 时间数据需要通过日志的ts时间戳转化得到，无法直接通过json字符串转化得到，需要在构造函数时通过获取的ts时间戳来进行格式化

#### 6.4.2 流的获取与转化

```scala
package com.orange.lin.app

import ...
/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-14-19:16
 */
object DauApp {

  //实现处理模块
  def main(args: Array[String]): Unit = {
    //TODO 1.创建StreamingContext
    val sparkConf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("DauApp")

    val ssc = new StreamingContext(sparkConf, Seconds(3))
    //TODO 2. 获取流
    val sourceStream: DStream[String] = MyKafkaUtil.getKafkaStream(ssc, Constant.STARTUP_TOPIC)
    // 2.1 把每个json字符串的数据,父封装到一个样例类对象中
    val startupLogStream: DStream[StartupLog] = sourceStream.map(json => JSON.parseObject(json, classOf[StartupLog]))


}
```

> 使用sparkStream至少需要2个核心，一个是采集器executor 另一个是driver

### 6.5 数据流的清洗与过滤

#### 6.5.1 日活业务的思路分析

日活定义：当天内有活跃的设备
需要考虑的点：当天同一台设备多次登录-->去重

sparkstreaming处理数据是一个批次一个批次处理的，如果使用filter直接去重，那么无法过滤整天的数据。会导致数据错误，其次去重要考虑批次问题。

##### 跨批次去重

- 方式一：updateStateByKey

作用说明：用于记录历史记录，给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对

> 注意：使用updateStateByKey需要对检查点目录进行配置，会使用检查点来保存临时状态。
>
> ![1594864315561](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233636.png)
>
> 使用updateStateByKey虽然可以实现状态的更新，但是使用有状态转换涉及到checkpoint的设置，需要使用HDFS来进行分布式存储，不同批次的用户数据都会存在于HDFS中，导致HDFS小文件过多

- 方式二：黑名单

> 黑名单方式需要利用外部存储结构，如MySQL、Redis
>
> MySQL的响应比较慢，此外对于日活的数据的统计更关注的是当天的情况，之前的数据没有保留的必要，redis有过期数据的方式可以自动删除无用数据，MySQL需要自己创建程序来周期型的删除，比较麻烦
>
> 使用redis的方式效率更好，响应速度更快
>
> **本次案例选用redis**

##### redis过滤机制的构建

- 创建Redis客户端的位置

![1594865818206](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233644.png)

> 分析：
>
> **客户端的创建如果放在位置1：**
>
> 程序无法执行，原因是数据库对象的连接一般都是安全连接，表示客户端与服务端建立的信任关系，如果把数据库对象序列化后交给别的executor执行，这种操作就不是安全操作了。
>
> **客户端的创建如果放在位置3：**
>
> 当把客户端连接放在该位置时，每个RDD的操作都会创建连接，导致性能严重下降
>
> **综上，选择位置2，该位置每个批次的数据获取一个连接对象进行黑名单判定**

当选择位置2作为创建客户端的操作时，就表示要将按批次处理的DStream转换为单个RDD来进行操作，因此需要选择 transform 转换因子

> 如果直接对Dstream使用map、filter转换算子，那么客户端创建的位置选择只有1和3。

##### 同批次去重

- 同一批次的跨天问题

以15号为例，该批次横跨了15号和16号，某一条数据流过来后，是拿16号的黑名单列表来判定该数据是否应该被过滤，当16号的黑名单数据没有相应设备id时，这一批次的数据无法过滤

- 在设备的首次启动时，如果同一个批次有设备的多次启动记录，则无法过滤

> 使用group by按设备id分组后，按时间排序取最新的数据的方式过滤

##### 整体去重思路

![1594870909711](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233733.png)



#### 6.5.2 代码实现

##### Redis客户端工具类的编写

```scala
package com.orange.lin.util

import redis.clients.jedis.{JedisPool, JedisPoolConfig}

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-15-11:31
 */
object RedisUtil {
  val conf =new JedisPoolConfig
  conf.setMaxTotal(100)
  conf.setMaxIdle(30)
  conf.setMinIdle(10)
  conf.setBlockWhenExhausted(true)
  conf.setMaxWaitMillis(10000)
  conf.setTestOnCreate(true)
  conf.setTestOnBorrow(true)
  conf.setTestOnReturn(true)
  val pool=new JedisPool(conf,"hadoop109",8000)


  def getClient() ={
    pool.getResource
  }
	//测试
  def main(args: Array[String]): Unit = {
    val client=getClient()
    client.set("k2","redis")
    client.close()
  }
}


```

> 使用从线程池获取连接的方式
>
> 注意：主机名和端口号实际上应该通过配置文件编写，否则端口和主机名会被写死

##### Redis的KV键值设计

| **key**            | **value** |
| ------------------ | --------- |
| **dau:2019-01-22** | 设备id    |

> redis只需要对设备id进行判断，因为数据判断按天统计指标，需要加上时间，时间为设备启动日志的时间，Dau为启动日志的主题名称

##### 去重代码实现

```scala
package com.orange.lin.app

import java.text.SimpleDateFormat
import java.util
import java.util.Date

import com.alibaba.fastjson.JSON
import com.orange.lin.Constant
import com.orange.lin.bean.StartupLog
import com.orange.lin.util.{MyKafkaUtil, RedisUtil}
import org.apache.spark.{SparkConf, rdd}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import redis.clients.jedis.Jedis

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-14-19:16
 */
object DauApp {

  //实现处理模块
  def main(args: Array[String]): Unit = {
    //TODO 1.创建StreamingContext
    val sparkConf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("DauApp")

    val ssc = new StreamingContext(sparkConf, Seconds(3))
    //TODO 2. 获取流
    val sourceStream: DStream[String] = MyKafkaUtil.getKafkaStream(ssc, Constant.STARTUP_TOPIC)
    // 2.1 把每个json字符串的数据,父封装到一个样例类对象中
    val startupLogStream: DStream[StartupLog] = sourceStream.map(json => JSON.parseObject(json, classOf[StartupLog]))
    //TODO 3. 去重  过滤掉一件启动的那些设备的记录  从redis去读取已经启动过的设备的id

    //TODO=============
    val filteredStartupLogStream: DStream[StartupLog] = startupLogStream.transform(rdd => {
      val client: Jedis = RedisUtil.getClient()
      val mids: util.Set[String] = client.smembers(Constant.STARTUP_TOPIC + ":" + new SimpleDateFormat("yyyy-MM-dd").format(new Date()))
      client.close()

      val midsBD: Broadcast[util.Set[String]] = ssc.sparkContext.broadcast(mids)
      //条件成立则保留，不成立则过滤
      rdd
        .filter(StartupLog => !midsBD.value.contains(StartupLog.mid))
        .map(log => (log.mid, log))
        .groupByKey()
        .map {
          case (_, logs) =>
            logs.toList.sortBy(_.ts).head
        }
    })
        ...
        ...
        ...

    //TODO 3. 开启流
    ssc.start()
    //TODO 4. 阻止main进程退出
    ssc.awaitTermination()
  }

}

```

### 6.6 保存到HBase与加入过滤用黑名单

> 使用Phoenix对HBase进行SQL化操作

#### 6.6.1 建表

- Phoenix登录

```shell
/opt/module/phoenix/bin/sqlline.py hadoop109,hadoop110,hadoop111:2181
```

- 建表

```sql
create table gmall2020_dau
(
              mid varchar,
              uid varchar,
              appid varchar,
              area varchar,
              os varchar,
              ch varchar,
              type varchar,
              vs varchar,
              logDate varchar,
              logHour varchar,
              ts bigint
              CONSTRAINT dau_pk PRIMARY KEY (mid, logDate));

```

#### 6.6.2 依赖增加

在pom.xml中

```xml
<dependency>
    <groupId>org.apache.phoenix</groupId>
    <artifactId>phoenix-spark</artifactId>
    <version>4.14.2-HBase-1.3</version>
</dependency>

<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
</dependency>

```

#### 6.6.3 实现代码(完整业务代码)

```scala
package com.orange.lin.app

import java.text.SimpleDateFormat
import java.util
import java.util.Date

import com.alibaba.fastjson.JSON
import com.orange.lin.Constant
import com.orange.lin.bean.StartupLog
import com.orange.lin.util.{MyKafkaUtil, RedisUtil}
import org.apache.spark.{SparkConf, rdd}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}
import redis.clients.jedis.Jedis

/**
 * @Description:
 * @author oranglzc
 * @creat 2020-07-14-19:16
 */
object DauApp {

  //实现处理模块
  def main(args: Array[String]): Unit = {
    //TODO 1.创建StreamingContext
    val sparkConf: SparkConf = new SparkConf().setMaster("local[2]").setAppName("DauApp")

    val ssc = new StreamingContext(sparkConf, Seconds(3))
    //TODO 2. 获取流
    val sourceStream: DStream[String] = MyKafkaUtil.getKafkaStream(ssc, Constant.STARTUP_TOPIC)
    // 2.1 把每个json字符串的数据,父封装到一个样例类对象中
    val startupLogStream: DStream[StartupLog] = sourceStream.map(json => JSON.parseObject(json, classOf[StartupLog]))
    //TODO 3. 去重  过滤掉一件启动的那些设备的记录  从redis去读取已经启动过的设备的id

    //TODO=============
    val filteredStartupLogStream: DStream[StartupLog] = startupLogStream.transform(rdd => {
      val client: Jedis = RedisUtil.getClient()
      val mids: util.Set[String] = client.smembers(Constant.STARTUP_TOPIC + ":" + new SimpleDateFormat("yyyy-MM-dd").format(new Date()))
      client.close()

      val midsBD: Broadcast[util.Set[String]] = ssc.sparkContext.broadcast(mids)
      //条件成立则保留，不成立则过滤
      rdd
        .filter(StartupLog => !midsBD.value.contains(StartupLog.mid))
        .map(log => (log.mid, log))
        .groupByKey()
        .map {
          case (_, logs) =>
            logs.toList.sortBy(_.ts).head
        }
    })

    filteredStartupLogStream.foreachRDD(rdd => {

      rdd.foreachPartition((logs: Iterator[StartupLog]) => {
        val client: Jedis = RedisUtil.getClient
        logs.foreach(log => {
          client.sadd(Constant.STARTUP_TOPIC + ":" + log.logDate, log.mid)
        })
        client.close()
      })
      import org.apache.phoenix.spark._
      //
      rdd.saveToPhoenix(
        "GMALL_DAU",
        Seq("MID", "UID", "APPID", "AREA", "OS", "CHANNEL", "LOGTYPE", "VERSION", "TS", "LOGDATE", "LOGHOUR"),
        zkUrl = Some("hadoop109,hadoop110,hadoop111:2181")
      )
    })
    filteredStartupLogStream.print(10000)

    //3.2

    //TODO 3. 开启流
    ssc.start()
    //TODO 4. 阻止main进程退出
    ssc.awaitTermination()
  }

}

```

> `saveToPhoenix`需要传入表名、表结构的字段名（需要一一对应），还需要传入`zookeeper`的集群地址
>
> Q:为什么需要传入zookeeper的集群URL？
>
> HBase的写流程中，连接HBase的客户端需要先访问zookeeper获取Hbase：meta位于哪个`region Server`，然后再访问对应的`Region Server`获取meta表，进而根据meta表找到目标table（写操作的目的地）





## 7 日活数据查询接口

### 7.1 访问路径

| 日活总数     | http://localhost:8070/realtime-total?date=2020-02-19        |
| ------------ | ----------------------------------------------------------- |
| **分时统计** | http://localhost:8070/realtime-hours?id=dau&date=2020-02-19 |

> 设置web服务器访问的端口：8070，请求获取的方式为get

### 7.2 请求的数据格式

| 总数      | [{"id":"dau","name":"新增日活","value":1200},   {"id":"new_mid","name":"新增设备","value":233}] |
| --------- | ------------------------------------------------------------ |
| 分时 统计 | {"yesterday":{"11":383,"12":123,"17":88,"19":200   },   "today":{"12":38,"13":1233,"17":123,"19":688   }} |

### 7.3 工程搭建

#### 7.3.1 创建工程

![1594878229857](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233805.png)

![1594878250584](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233819.png)

选择mybatis、JDBC、和Spring Web组件

![1594878303946](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233825.png)

#### 7.3.2 配置文件

##### 子模块`gmall-publisher`依赖配置

- 修改父依赖
- 增加phoenix查询所需要的工具jar包
- 增加hadoop依赖

> 解决网页打开异常

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <artifactId>realtime-gmall</artifactId>
        <groupId>com.orange.lin</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <groupId>com.orange.lin</groupId>
    <artifactId>gmall-publisher</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>gmall-publisher</name>
    <description>Demo project for Spring Boot</description>

    <properties>
        <java.version>1.8</java.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-jdbc</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.mybatis.spring.boot</groupId>
            <artifactId>mybatis-spring-boot-starter</artifactId>
            <version>2.1.3</version>
        </dependency>
        <dependency>
            <groupId>com.orange.lin</groupId>
            <artifactId>gmall-common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.junit.vintage</groupId>
                    <artifactId>junit-vintage-engine</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <!--phoenix-->
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-core</artifactId>
            <version>5.0.0-HBase-2.0</version>
        </dependency>
        <!--phoenix 用到的工具包-->
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
            <version>20.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>3.1.3</version>
        </dependency>
        <dependency>
            <groupId>org.junit.jupiter</groupId>
            <artifactId>junit-jupiter</artifactId>
            <version>RELEASE</version>
            <scope>test</scope>
        </dependency>

    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>

</project>

```

##### 父依赖配置

```xml
...
    <modules>
        <module>gmall-common</module>
        <module>gmall-mock</module>
        <module>gmall-logger</module>
        <module>gmall-realtime</module>
        <module>gmall-publisher</module>
    </modules>
...
```

##### `gmall-publisher`配置文件

application.properties

```properties
#mybatis配置
server.port=8070
#logging.level.root=error
# jdbc
spring.datasource.driver-class-name=org.apache.phoenix.jdbc.PhoenixDriver
spring.datasource.url=jdbc:phoenix:hadoop109,hadoop110,hadoop111:2181


# mybatis
# 做映射的时候会在 resources/mapper 目录找对应的 xml 文件
mybatis.mapperLocations=classpath:mapper/*.xml
# 字段名是否自动从下划线映射到驼峰命名  一般都是使用 true
mybatis.configuration.map-underscore-to-camel-case=true
```

### 7.4 代码实现

#### 7.4.1 MVC分层设计

| 控制层               | PublisherController       | 实现接口的web发布     |
| -------------------- | ------------------------- | --------------------- |
| 服务层               | PublisherService          | 数据业务查询interface |
| PublisherServiceImpl | 业务查询的实现类          |                       |
| 数据层               | DauMapper                 | 数据层查询的interface |
| DauMapper.xml        | 数据层查询的实现配置      |                       |
| 主程序               | GmallPublisherApplication | 增加扫描包            |

> 本项目module结构：
>
> ![1594878918146](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200719233836.png)

#### 7.4.2 GmallPublisherApplication（程序入口）

```java
package com.orange.lin.gmallpublisher;

import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
@MapperScan(basePackages = "com.orange.lin.gmallpublisher.mapper")
public class GmallPublisherApplication {

    public static void main(String[] args) {
        SpringApplication.run(GmallPublisherApplication.class, args);
    }

}

```

> 在SpringBoot中集成MyBatis，可以在mapper接口上添加@Mapper注解，将mapper注入到Spring,但是如果每一个mapper都添加@mapper注解会很麻烦，这时可以使用@MapperScan注解来扫描包。
>
> **@MapperScan("com.demo.mapper")：扫描指定包中的接口**
>
> **注意扫描的是包**

#### 7.4.3 Mapper数据层

##### Mapper层接口类

```
package com.orange.lin.gmallpublisher.mapper;

import java.util.List;
import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-15:49
 */
public interface DauMapper {

    //得到总的日活
    Long getDau(String date);

    //得到日活分时
    List<Map<String,Object>> getHourDau(String date);
}

```

##### 数据层返回值类型的确定

- 对于日活来说，查询后的结果是一个聚合值，返回的是一个Long型的

- 对于日活分时统计结果来说，返回的是一个表结构，涉及到表结构-->java数据类型的转化

> 因此在xml配置中，不可以使用`resultType`直接指定返回值类型
>
> ```
>     +----------+-----------+
>     | LOGHOUR  | COUNT(1)  |
>     +----------+-----------+
>     | 10       | 44        |
>     | 15       | 140       |
>     +----------+-----------+
> 对于返回的表结果集，使用list表示整个表，每一行数据为一个map，map中有2个K-V键值对表示，分别是字段名->值
> ```
>
> 使用resultMap表示返回的类型是list，通过
>
> ```xml
>  <resultMap id="hourDauList" type="java.util.Map"></resultMap>
> ```
>
> 指明包含的关系是map

##### 数据层xml配置

```xml
<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE mapper SYSTEM "http://mybatis.org/dtd/mybatis-3-mapper.dtd" >
<!--namespace 对应前面定义的接口-->
<mapper namespace="com.orange.lin.gmallpublisher.mapper.DauMapper">
    <!--对应前面接口中的方法,  标签内写响应的查询语句, 查询的接口会赋值给这个方法的返回值-->
    <select id="getDau" resultType="java.lang.Long">
        select count(1) from gmall_dau where logdate=#{date}
    </select>
    <select id="getHourDau" resultMap="hourDauList">
        select LOGHOUR, COUNT(*) COUNT from GMALL_DAU where LOGDATE=#{date} group by LOGHOUR
    </select>
    <resultMap id="hourDauList" type="java.util.Map"></resultMap>


</mapper>

```

> mybatis会通过XML的配置和接口类定义的抽象方法自动获取包含结果集的实现类，用户字需要通过接口函数确定方法名和数据返回结果，通过xml配置文件指定接口类地址、方法名和sql语句即可
>
> 变量的写法： #{}



#### 7.4.4 Service服务层

##### service层接口类

```java
package com.orange.lin.gmallpublisher.service;

import org.springframework.stereotype.Service;

import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:04
 */

public interface PublisherService {
    //获取总得日活
    Long getDau(String date);
       /*
        数据层
        // List(Map("loghour": "10", count: 100), Map,.....)
        List<Map<String, Object>> getHourDau(String date);

        //  Map("10"->100, "11"->200. "12"->100)
     */
    Map<String, Long> getHourDau(String date);

}

```

##### service层实现类

```java
package com.orange.lin.gmallpublisher.service;

import com.orange.lin.gmallpublisher.mapper.DauMapper;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:05
 */
@Service
public class PublisherServiceImp implements PublisherService {

    @Autowired
    DauMapper dau;

    @Override
    public Long getDau(String date) {
        return dau.getDau(date);
    }
    /*
  数据层
  // List(Map("loghour": "10", count: 100), Map,.....)
  List<Map<String, Object>> getHourDau(String date);
  select LOGHOUR, count(*) COUNT from GMALL_DAU where LOGDATE=#{date } group by LOGHOUR

  //  Map("10"->100, "11"->200. "12"->100)

	*/
    @Override
    public Map<String, Long> getHourDau(String date) {

        List<Map<String, Object>> hourDau = dau.getHourDau(date);

        Map<String, Long> result = new HashMap<>();

        for (Map<String, Object> map : hourDau) {
            String key = map.get("LOGHOUR").toString();
            Long value = (Long) map.get("COUNT");
            result.put(key, value);
        }
        return result;
    }
}

```

> 数据结构的转化，将表结构的关系（原本的数据只是存在于同一个map中），需要转化成 value与value的一一对应关系



#### 7.4.5 controller控制层

```java
package com.orange.lin.gmallpublisher.controller;

import com.alibaba.fastjson.JSON;
import com.orange.lin.gmallpublisher.service.PublisherService;
import it.unimi.dsi.fastutil.Hash;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import java.time.LocalDate;
import java.util.*;

/**
 * @author oranglzc
 * @Description:
 * @creat 2020-07-15-16:07
 */
@RestController
public class publisherController {
    @Autowired
    PublisherService service;

    /*
    1.	日活总数:
http://localhost:8070/realtime-total?date=2020-02-11

    2.	日活总数
[{"id":"dau","name":"新增日活","value":1200},
{"id":"new_mid","name":"新增设备","value":233} ]
    */
    @GetMapping("/realtime-total")
    public  String realtimeTotal(String date){
        Long dau = service.getDau(date);
        // json字符串先用java的数据结构表示, 最后使用json序列化工具直接转成json字符串
        List<Map<String,String>> result = new ArrayList<>();

        HashMap<String, String> map1 = new HashMap<>();

        result.add(map1);
        map1.put("id", "dau");
        map1.put("name", "新增日活");
        map1.put("value", dau.toString());

        Map<String, String> map2 = new HashMap<>();
        result.add(map2);
        map2.put("id", "new_mid");
        map2.put("name", "新增设备");
        map2.put("value", "233");



        return JSON.toJSONString(result);
    }
    
    /*
    
	1.	日活分时统计
http://localhost:8070/realtime-hour?id=dau&date=2020-07-15
    2.	日活分时统计
{"yesterday":{"11":383,"12":123,"17":88,"19":200 },
"today":{"12":38,"13":1233,"17":123,"19":688 }}
    */
    @GetMapping("/realtime-hour")
    public String getRealTimeHour(String id,String date){
        if ("dau".equals(id)){
            Map<String, Long> today = service.getHourDau(date);
            Map<String, Long> yesterday = service.getHourDau(getYesterday(date));

            HashMap<String , Map<String,Long>> result = new HashMap<>();
            result.put("today", today);
            result.put("yesterday", yesterday);

            return JSON.toJSONString(result);
        }else {
            return  null;
        }

    }
    /**
     * 返回昨天的年月日
     *
     * @param date
     * @return
     */
    private String getYesterday(String date) {
        return LocalDate.parse(date).plusDays(-1).toString();
    }


}

```

## 8 可视化展示

无















