# 模块总结

***

## 一、业务数据如何到mysql中

```sql
在mysql中创建对应的数据表，然后使用自定义一个函数，自动生成数据。
# 参数为：日期  订单个数 用户数 是否删除以前的数据
call init_data("2019-05-16", 10,2,false)
```

## 二、canal监控mysql数据发送到kafka

```scala
import java.net.{InetSocketAddress, SocketAddress}
import java.util

import com.alibaba.fastjson.JSONObject
import com.alibaba.otter.canal.client.{CanalConnector, CanalConnectors}
import com.alibaba.otter.canal.protocol.CanalEntry.{EventType, RowChange}
import com.alibaba.otter.canal.protocol.{CanalEntry, Message}
import com.atguigu.common.Constant
import com.google.protobuf.ByteString

import scala.collection.JavaConversions._

/**
  * @Description 创建canal的客户端
  **
  * @author lianzhipeng
  * @create 2020-07-17 15:38:41
  */
object CanalClient {

  /**
    * 解析数据
    * @param rowDatas 表示一个sql语句导致改变的数据,有多行数据
    * @param TableName 表示这些数据的所在的表
    * @param eventType 表示sql语句的类型，有insert、upda等
    */
  def handleData(rowDatas: util.List[CanalEntry.RowData], TableName: String, eventType: CanalEntry.EventType) = {
    // 我们只想获取order_info表中的数据，其他表的数据不想处理，同时只想获取新增的数据，新增的数据表示新增的订单的数据
    if(TableName.equals("order_info") && eventType == EventType.INSERT && rowDatas != null && !rowDatas.isEmpty){
     
      handleMysqlDataToKafka(rowDatas,Constant.TOPIC_ORDER_INFO)
      
    }else  if(TableName.equals("order_detail") && eventType == EventType.INSERT && rowDatas != null && !rowDatas.isEmpty){
      
      handleMysqlDataToKafka(rowDatas,Constant.TOPIC_ORDER_DETAIL)
      
    }

  }

  /**
    * 解析mysql变化的数据，并写到kafka中
    * @param rowDatas 表示一个sql语句导致改变的数据,有多行数据
    * @param topic kafka的topic
    */
  private def handleMysqlDataToKafka(rowDatas: util.List[CanalEntry.RowData],topic:String)= {
    // 遍历多行数据,获取一条一条的数据
    for (rowdata <- rowDatas) {
      // 每一条数据由很多列组成。获取了这一行数据的多列，getAfterColumnsList，表示获取变化后的数据
      val columnsList: util.List[CanalEntry.Column] = rowdata.getAfterColumnsList
      // 创建一个json对象，来接收这一行的数据，mysql中的一行数据到kafka中保存为一条json数据
      val obj = new JSONObject()
      // 遍历这些列
      for (column <- columnsList) {
        // 获取列名
        val key: String = column.getName
        // 获取这一列的值
        val value: String = column.getValue
        obj.put(key, value)
      }
      // println(obj)
      // 将json数据转换为字符串
      val context: String = obj.toJSONString
      println(context)
      // 将获取的数据发往kafka中,指明发往哪个topic中和发送什么数据
      KafkaUtils.sentToKafka(topic, context)
    }
  }

  def main(args: Array[String]): Unit = {
    // 11111是canal的端口
    var address: SocketAddress =new InetSocketAddress("hadoop105",11111)
    // 1. 连接canal，"example"是/opt/module/canal/conf/example,
    // 1.1 在该目录下有一个配置文件，配置文件中记录了需要监控的mysql地址和数据库，当前是监控gmall0213数据库下所有的表
    val connector: CanalConnector = CanalConnectors.newSingleConnector(address,"example","","")
    // 1.2 连接canal
    connector.connect()
    // 2. 拉取数据
    // 2.1 canal客户端订阅canal的数据,订阅canal监控的gmall0213数据库下的所有的表
    connector.subscribe("gmall0213.*")
    // 死循环来不断拉取数据
    while (true){
      /*
       1. canal的客户端获取canal监控的数据
       2. 100表示最多由100条sql导致改变的数据，这100条数据封装成一个message信息
       3. 使用死循环来不断的拉取数据
        */
      val message: Message = connector.get(100)

      // 一个message封装了多个entry，获取entry
      val entries: util.List[CanalEntry.Entry] = message.getEntries
      // 一条entry表示一条sql导致的数据的变化
      // 如果拉取的数据不为空，那么继续解析数据，如果拉取到的数据为空，那么就等3秒再拉数据
      //  同时进行非空判断，来避免空指针的问题。
      if(entries!=null && !entries.isEmpty){
        // 遍历entries，获取一个一个entry
      for(entry <- entries){
        // 对一个entry进行非空判断和entry的类型进行判断
        if(entry != null && entry.hasEntryType && entry.getEntryType == CanalEntry.EntryType.ROWDATA ){
          // 一个entry是一个sql语句导致的变化的数据，一个entry封装成了一个storevalue，获取storevalue
          val storeValue: ByteString = entry.getStoreValue
          // 一个storeValue封装成一个rowchange,一个rowchange表示这个sql语句变化的多行数据
          val rowChange: RowChange = RowChange.parseFrom(storeValue)
          // 获取多行数据
          val rowDatas: util.List[CanalEntry.RowData] = rowChange.getRowDatasList
          // 封装一个方法，来处理一个sql语句导致变化的多行数据
          // rowdatas表示多行数据
          // entry.getHeader.getTableName:表示canal是监控gmall2013数据库中的所有表
          // rowChange.getEventType:表示数据被改变的方式，有insert，update、等等
          handleData(rowDatas,entry.getHeader.getTableName,rowChange.getEventType)

          }
        }
      }else {
        System.out.println("没有拉取到数据，3s后继续拉取....")
        // 拉取数据的操作等待3s后再执行
        Thread.sleep(3000)
      }
    }
  }
}

```

## 三、往kafka中写数据

```scala
import java.util.Properties
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}

/**
  * @Description 定义一个kafka的工具类
  * @author lianzhipeng
  * @create 2020-07-17 18:07:07
  */
object KafkaUtils {

  var pops = new Properties()
  // 设定kafka集群的地址
  pops.setProperty("bootstrap.servers","hadoop105:9092,hadoop106:9092,hadoop107:9092")
  // 设定key的序列化器
  // 设定value的序列器
  pops.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
  pops.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

  val producer = new KafkaProducer[String,String](pops)

  def sentToKafka(topic: String, context: String) = {
    producer.send(new ProducerRecord[String,String](topic,context))
  }
}
```

## 四、sparkStreaming读取kafka中的数据

```sql
步骤说明：
1. 创建kafkaParams参数，参数中包含了
   1. kafka集群地址
   2. key和value的序列化器
   3. 消费者组id
   4. 消费的offset
   5. offset是否自动提交
2. 使用kafkautils创建直连的方式，sparkstreaming来连接kafka
3. 需要传递的参数有：ssc(streamingcontext),一致性，订阅的topic
4. 对获取的输入流使用map进行处理，只要value，不要key
```

```scala
package com.atguigu.realtime.utils


import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

/**
  * @Description
  * @author lianzhipeng
  * @create 2020-07-14 18:29:12
  */
object MyKafkaUtil {

  val kafkaParams = Map[String, Object](
    "bootstrap.servers" -> ConfigUtils.getProperties("gmall.properties", "bootstrap.servers"),
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    "group.id" -> ConfigUtils.getProperties("gmall.properties", "group.id"),
    "auto.offset.reset" -> "latest",
    "enable.auto.commit" -> (true: java.lang.Boolean)
  )

  def getKafkaStream(ssc: StreamingContext, topic: String) = {

    KafkaUtils.createDirectStream[String, String](
      ssc,
      PreferConsistent,
      Subscribe[String, String](Set(topic), kafkaParams)
    ).map(_.value())

  }

}

```

## 五、如何读取配置文件中的参数

```scala
package com.atguigu.realtime.utils

import java.io.InputStream
import java.util.Properties

/**
  * @Description
  **
  * @author lianzhipeng
  * @create 2020-07-15 11:06:43
  */
object ConfigUtils {

  def getProperties(fileName: String,configName:String) ={

    // 1. 获取类的加载器并获取一个资源输入流
    val is: InputStream = ConfigUtils.getClass.getClassLoader.getResourceAsStream(fileName)
    // 2. 获取一个配置文件的对象
    val ps = new Properties()
    // 3. 配置文件对象加载这个输入流
    ps.load(is)
    // 4. 配置对象可以获取配置文件中的配置信息
    ps.getProperty(configName)

  }

  def main(args: Array[String]): Unit = {
    // 测试是否能获取到配置信息
    println(getProperties("gmall.properties", "bootstrap.servers"))
   // hadoop105:9092,hadoop106:9092,hadoop107:9092
  }

}

```

## 六、公共的代码的封装

```sql
将sparkstreaming公共的代码进行封装成一个trait，创建的sparkstreaming类只要要继承这个trait，实现run方法。
封装的内容有：
1. sparkstreaming环境对象streamingcontext创建
2. 采集器开启
3. 线程的阻塞
4. run方式就是获取流以后的具体数据处理
```



```scala
package com.atguigu.realtime.app

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
  * @Description
  **
  * @author lianzhipeng
  * @create 2020-07-17 18:35:06
  */
trait BaseAPP {

  def run(ssc: StreamingContext) ;

  def main(args: Array[String]): Unit = {

    val SparkConf: SparkConf = new SparkConf().setAppName("OrderApp").setMaster("local[*]")
    val ssc = new StreamingContext(SparkConf, Seconds(3))
    run(ssc)
    // 6. 启动采集器
    ssc.start()
    // 7. 阻塞main进程停止
    ssc.awaitTermination()

  }
  
}

```

## 七、sparkstreaming双流join

```scala
package com.atguigu.realtime.app
import java.util

import com.alibaba.fastjson.JSON
import com.atguigu.common.Constant
import com.atguigu.realtime.bean.{OrderDetail, OrderInfo, SaleDetail, UserInfo}
import com.atguigu.realtime.utils.{EsUtil, MyKafkaUtil, MyslqUtil, RedisUtils}

import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream
import org.json4s.DefaultFormats
import org.json4s.jackson.Serialization
import redis.clients.jedis.Jedis


/**
  * @Description
  **
  * @author lianzhipeng
  * @create 2020-07-21 16:05:41
  */
object SaleApp extends BaseAPP {

  override def run(ssc: StreamingContext): Unit = {

      // 1. 定义一个方法，用来获取两个流
      val (infoStream,detailStream) = getOrderInfoAndOrderDetailStream(ssc)

      // 2. 双流join
     val fullJoinStream: DStream[SaleDetail] = fullJoin(infoStream,detailStream)

     // 3. 在数据中添加user_info数据
    val SaleDetailStream: DStream[SaleDetail] = addUserInfo(fullJoinStream,ssc)

     // 4. 将结果写到es中
    sendToEs(SaleDetailStream)
   // 5. 测试
    SaleDetailStream.print(1000)


  }

 
  /**
    * 将数据写到es中，通过获取已有数据的uid，然后取mysql中读取数据，并放置流中
    * @param fullJoinStream
    */
  def addUserInfo(fullJoinStream: DStream[SaleDetail],ssc:StreamingContext) = {

    fullJoinStream.transform( rdd => {

      // 1. 读取指定mysql表中数据
      val Readrdd: RDD[UserInfo] = MyslqUtil.ReadMySQLData(ssc,"user_info")
      // 2. 对数据进行转换,用于和原来的RDD进行join
      val useInfoRDD: RDD[(String, UserInfo)] = Readrdd.map(user => (user.id,user))
      // 3. 将saleDetail转换成kv，然后和user 进行join，补齐user的信息
      val detailRDD: RDD[(String, SaleDetail)] = rdd.map(saleDetail => (saleDetail.user_id,saleDetail))
      // 4. RDD进行join
      detailRDD.join(useInfoRDD)
        .map{
          case (userId,(saleDetail,userInfo)) =>{
            saleDetail.mergeUserInfo(userInfo)
          }
      }

    })


  }


  /**
    * @param infoStream
    * @param detailStream
    */
  def fullJoin(infoStream: DStream[OrderInfo], detailStream: DStream[OrderDetail])= {
    // 将info的数据缓存到redis中
    def cacheOrderInfoToRedis(client: Jedis, orderInfo: OrderInfo): String = {
      /**
        * orderinfo在redis中的缓存信息
        * key : "order_info :" + orderInfo.id
        * value : orderInfo一条数据
        */
      client.set("order_info :" + orderInfo.id, Serialization.write(orderInfo)(DefaultFormats))
    }

    def cacheOrderDetailToRedis(client: Jedis, orderDetail: OrderDetail): String = {
      client.set("orderDetail:" + orderDetail.order_id + ":" + orderDetail.id, Serialization.write(orderDetail)(DefaultFormats))
    }

    // 1. 对rdd进行转换，因为只有kv类型的数据才能进行join
    val mapInfoStream: DStream[(String, OrderInfo)] = infoStream.map(info => (info.id,info))
    val mapDetailStream: DStream[(String, OrderDetail)] = detailStream.map(detail => (detail.order_id,detail))
    // 2. 对流进行join，join时是按照相同key会join在一起。而且是一条一条的join,一个订单会对应多个订单详情表
    // (order_id1,(订单1，详情1))，(order_id1,(订单1，详情2))
    // 为了提高效率，使用一个分区的数据同时进行计算

    mapInfoStream.fullOuterJoin(mapDetailStream).mapPartitions(it => {

      // 1. 获取redis的连接，因为需要从redis中获取数据
      val client: Jedis = RedisUtils.getClient

      val result = it.flatMap {
        // info和detail都有数据
        case (order_id, (Some(orderInfo), Some(orderDetail))) =>
          // 1. 将info数据写到缓存中,orderinfo是对象，需要解析为json字符串
          cacheOrderInfoToRedis(client, orderInfo)
          // 2. 将数据进行合并
          val infoDetail: SaleDetail = SaleDetail().mergeOrderInfo(orderInfo).mergeOrderDetail(orderDetail)
          // 3. 从redis中获取detail的数据
          /**
            * orderDetails在redis中的缓存信息
            * key ： orderDetail + orderInfo.id + orderDetail.id
            * value: orderDetail一条数据
            */
          val details: util.Set[String] = client.keys("orderDetail:" + orderInfo.id + ":*")
          // 创建一个集合来接收detalis
          import scala.collection.JavaConversions._
          var saleDetails = new util.ArrayList[SaleDetail]()
          saleDetails.add(infoDetail)
          for (detail <- details) {
            // 将字符串转换为样例类
            val obj: OrderDetail = JSON.parseObject(detail, classOf[OrderDetail])
            // 从缓存中删除已经被使用的数据
            client.del(detail)
            // 将数据进行合并
            val saleDetail = SaleDetail().mergeOrderInfo(orderInfo).mergeOrderDetail(obj)
            saleDetails.add(saleDetail)
          }
          saleDetails

        /**
          * orderinfo有数据，但是detail没有数据
          */
        case (order_id, (Some(orderInfo), None)) =>
          // 1. 将info数据写到缓存中,orderinfo是对象，需要解析为json字符串
          cacheOrderInfoToRedis(client, orderInfo)
          // 2. 从redis中获取detail的数据
          /**
            * orderDetails在redis中的缓存信息
            * key ： orderDetail + orderInfo.id + orderDetail.id
            * value: orderDetail一条数据
            */
          val details: util.Set[String] = client.keys("orderDetail:" + orderInfo.id + ":*")
          // 创建一个集合来接收detalis
          import scala.collection.JavaConversions._
          var saleDetails = new util.ArrayList[SaleDetail]()
          for (detail <- details) {
            // 将字符串转换为样例类
            val obj: OrderDetail = JSON.parseObject(detail, classOf[OrderDetail])
            // 从缓存中删除已经被使用的数据
            client.del(detail)
            // 将数据进行合并
            val saleDetail = SaleDetail().mergeOrderInfo(orderInfo).mergeOrderDetail(obj)
            saleDetails.add(saleDetail)
          }
          saleDetails

        case (order_id, (None, Some(orderDetail))) =>
          // 1. 从redis缓存中读取info的数据
          val value: String = client.get("order_info :" + orderDetail.order_id)
          if (value == null) {
            // 将detail的数据写入缓存中
            cacheOrderDetailToRedis(client, orderDetail)
            List()
          } else {
            val info = JSON.parseObject(value, classOf[OrderInfo])
            val saleDetail = SaleDetail().mergeOrderInfo(info).mergeOrderDetail(orderDetail)
            List(saleDetail)
          }
      }
      client.close()
      result

    })


  }


  /**
    * 获取info流和detail流
    * @param ssc streamingcontext对象
    * @return
    */
  def getOrderInfoAndOrderDetailStream(ssc: StreamingContext) = {
    // 第一步：获取topic_order_info的流,并将数据封装成样例类
    val infoStream: DStream[OrderInfo] = MyKafkaUtil.getKafkaStream(ssc,Constant.TOPIC_ORDER_INFO)
      .map(json => JSON.parseObject(json,classOf[OrderInfo]))
    // 第二步：获取topic_order_detail的流，并将数据封装成样例类
    val detailStream: DStream[OrderDetail] = MyKafkaUtil.getKafkaStream(ssc, Constant.TOPIC_ORDER_DETAIL)
      .map(json => JSON.parseObject(json, classOf[OrderDetail]))
    (infoStream,detailStream)
  }
}

```



## 八、sparkstreaming的数据写到mysql

```

```



## 九、sparkstreaming的数据写到elasticsearch

```scala
package com.atguigu.realtime.utils

import io.searchbox.client.{JestClient, JestClientFactory}
import io.searchbox.client.config.HttpClientConfig
import io.searchbox.core.{Bulk, Index}

/**
  * @Description
  **
  * @author lianzhipeng
  * @create 2020-07-20 14:58:11
  */
object EsUtil {

  /**
    *  1. 获取es的客户端
    *     a、创建一个工厂对象
    *     b、es的配置参数
    *     c、通过工厂对象 + 配置参数 创建es的对象
    *
    */
  val factory = new JestClientFactory
  // es的url
  val esurl = "http://hadoop105:9200"
  val conf: HttpClientConfig = new HttpClientConfig.Builder(esurl)
    .connTimeout(1000 * 10) //连接延迟时间
    .readTimeout(1000 * 10) // 获取数据延迟时间
    .maxTotalConnection(100) // 最大的连接数
    .multiThreaded(true) // 是否启动多线程
    .build()

  factory.setHttpClientConfig(conf)


  /**
    * 向es中添加多条数据
    * @param index index
    * @param sources 多条数据源
    */
  def insertBulk(index:String,sources:Iterator[Object]) ={
    // 创建es的客户端
    val client: JestClient = factory.getObject
    // 构建一个bulk，配置index和type
    var bulkBuilder: Bulk.Builder = new Bulk.Builder()
      .defaultIndex(index)
      .defaultType("_doc")
    // 遍历多个数据源，使用模式匹配的方式
    sources.foreach{
      // 如果传了document_id，则使用，如果没有传递，则随机生成一个document_id
      case (id :String,source) =>
        val action = new Index.Builder(source).id(id).build()
        // 添加action
        bulkBuilder.addAction(action)
      case source =>
        val action: Index.Builder = new Index.Builder(source)
        bulkBuilder.addAction(action.build())

  }
    // 执行
    client.execute(bulkBuilder.build())
    // 关闭客户端
    client.shutdownClient()

  }

  /**
    * 往es中写单条数据
    * @param index index
    * @param source 数据源，具体添加的数据
    * @param id document_id
    *
    */
  def insertSingle(index :String,source:Object,id :String=null): Unit = {

    val client: JestClient = factory.getObject

    /**
      * 2. 往es中写数据
      */
    val action: Index = new Index.Builder(source)
      .index(index)
      .`type`("_doc")
      .id(id)
      .build()

    client.execute(action)

    /**
      * 3. 关闭客户端的连接
      */
    client.shutdownClient()
  }
}
case  class  User(name :String,age : Int)

/**
    * 将封装好的数据发往es中
    * @param fullJoinStream
    */
  def sendToEs(fullJoinStream: DStream[SaleDetail]) = {
    // 1. 将数据写到es中
    fullJoinStream.foreachRDD(rdd => {

      rdd.foreachPartition(
        it => {
          EsUtil.insertBulk("gmall_sale_detail", it.map(detail => (detail.order_id + "_" + detail.order_detail_id, detail)))
        }
      )

    })

  }
```



## 十、sparkstreaming的数据写到hbase

```scala
 // 5. 将新增mid的详情写到hbase中。
      import  org.apache.phoenix.spark._
      rdd.saveToPhoenix(
        "GMALL_DAU",
        Seq("MID", "UID", "APPID", "AREA", "OS", "CHANNEL", "LOGTYPE", "VERSION", "TS", "LOGDATE", "LOGHOUR"),
        zkUrl = Some("hadoop105,hadoop106,hadoop107:2181"))
    })
```



## 十一、如何连接redis

```scala
package com.atguigu.realtime.utils

import redis.clients.jedis.{Jedis, JedisPool, JedisPoolConfig}

/**
  * @Description 获取与redis的连接
  **
  * @author lianzhipeng
  * @create 2020-07-15 16:24:08
  */
object RedisUtils {

  val conf = new JedisPoolConfig
  conf.setMaxIdle(30) //连接池中最大的空闲连接数量
  conf.setMinIdle(10) //连接池中最小的空闲连接数量
  conf.setMaxTotal(100) //最大的连接数量
  conf.setBlockWhenExhausted(true) // 没有可用的连接时是否等待
  conf.setMaxWaitMillis(10000) //最大的等待时间
  conf.setTestOnCreate(true) // 测试是否连接
  conf.setTestOnBorrow(true) // 每次获得连接的进行测试
  conf.setTestOnReturn(true)// 验证连接池的属性
  val pool = new JedisPool(conf,"hadoop105",8000)

  def getClient = {
    // 从连接池中获取一个连接
    pool.getResource

  }
   /*
   测试是否能够连接redis并且是否可以写数据
    */
  def main(args: Array[String]): Unit = {
    // 获取一个客户端
    val client: Jedis = getClient
    // 使用客户端往redis中写数据
    client.set("k2","lianzp")
    // 关闭连接
    client.close()
    // 关闭连接池
    pool.destroy()
  }

}

```



## 十二、SparkStreaming如何连接mysql

```scala
package com.atguigu.realtime.utils

import java.util.Properties

import com.atguigu.realtime.bean.UserInfo
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.StreamingContext

/**
  * @Description SparkStreaming使用jdbc的方式连接mysql
  **
  * @author lianzhipeng
  * @create 2020-07-21 20:27:52
  */
object MyslqUtil {
  val url = ConfigUtils.getProperties("gmall.properties","url")
  val password =  ConfigUtils.getProperties("gmall.properties","password")
  val user =  ConfigUtils.getProperties("gmall.properties","user")
  val props = new Properties()
  props.setProperty("user", user)
  props.setProperty("password", password)

  def ReadMySQLData(ssc : StreamingContext,tableName: String) ={
      // 构建sparkSession对象
      val spark: SparkSession = SparkSession.builder()
        .config(ssc.sparkContext.getConf)
        .getOrCreate()
      import spark.implicits._
    // 读取指定路径下的数据，默认按行读取
      spark.read.jdbc(url, tableName, props)
        .as[UserInfo]
        .rdd
    }

}

```



## 十二、es中的数据如何写到前端

### 1. 需求分析

```sql
1. '思想'：将数据写给前端，首先需要通过mybaits创建一个接口，前端通过这个接口，来访问数据。
2. '实现步骤'：
   1. 获取es的客户端
   2. 创建查询的对象
   3. 执行查询
   4. 获取数据并对数据进行解析
   5. 将解析后的数据发送到网页上进行展示
   6. 前端对接发送到网页中的数据，实现数据展示。
```

- 第一步：需求分析

```sql
-- 1. '用户的需求'：
       用户输入关键词和时间，查询这天的
       1. 男女比例：男生和女生的比例
       2. 年龄比例数据：不同年龄段的用户数量
       3. 同时展示当天的数据明细。
-- 2. '案例'：
       如用户查询：2020-07-21这天，小米手机订单销售情况，需要的数据有：
       1. 男女比例
       2. 购买的年龄端分布情况
       3. 订单详情信息。
-- 3. 实现方式
       1. 如何获取es中的数据
          a、连接es
          b、通过dsl的语法来获取数据
       2. 如何将获取的数据转换为对应的数据结构 
          数据包含：
          a、男女比例的数据
          b、分年龄段聚合的数据
          c、订单详情的数据明细
       3. 数据如何发送到网页中
          a、通过mybaits的数据的相应的方式。
```

![image-20200722144734688](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722144734.png)

![image-20200722144700964](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722144708.png)

```sql
-- 4. 在kibana中使用dsl实现如上的需求为：
     '需求1：统计男女比例数据分析'：
     GET  gmall_sale_detail/_search
{
  "query": {
    "bool": {
      "filter": {
        "term": {
          "dt": "2020-07-22"  -- 获取哪一天的数据
        }
      }
      , "must": [
        {"match": {
          "sku_name": "小米手机" -- 查询产品，是模糊查询，采用了中文ik的分词器
        }}
      ]
    }
  },
  "aggs": {
    "group_by_user_gender": {  -- 分组的名字
      "terms": {
        "field": "user_gender", -- 按照什么进行分组
        "size": 2               -- 分组后的条目数量，假如可以分为10个组时，设定分组size为2，则剩余8个组为进入其他组
      }
    }
  },
  "from": 0,    -- 数据展示时，会将聚合前数据进行展示，也就是过滤出来数据，from表示从几个开始展示
  "size": 3     --  表示从from开始，展示多少条数据
}

  '需求2'：统计购买这些产品不同年龄端的数据。我们的做法是：
          通过dsl的查询语句，得到同一天购买指定商品的同一年龄的数据，如：
          2020-07-21，购买有小米手机关键字的订单中各个年龄的数据。
          等待拿到了这个数据以后，再对数据进行区间判断，计算出每个年龄端的数量。
```

<img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722182739.png" alt="image-20200722182738113" style="zoom:50%;" />

### 2. 具体步骤

#### 2.1 第一步：获取es的客户端、解析数据、并将数据写给controller

在==gmall-publisher==项目中的==service==包中==PublisherService接口==中定义一个==抽象方法==，指定获取数据类型以及需要传递的参数

```sql
-- 1. 参数分析：见参数说明
-- 2. 返回值类型说明：
      1.'希望返回的数据'：
            1. 查询数据的总数量：“total”：4995
            2. 分组后聚合的数据：
                    男女的聚合数据：genderAgg：Map("f" -> 2381,"M"->2614) '性别-> 数量'
                    年龄分布聚合数据：ageAgg ：Map(20->3,21->20) '年龄->数量'
            3. 详情数据：detail:List(Map(一条数据)，Map(一条数据))
                    一条数据：
                    order_detail_id -> "30293"
                    user_age -> 17
       2. '那么这些数据我们都需要'：
            男女的聚合数据:total + gendetAgg + detail
            年龄分布聚合数据:total + ageAgg + detail
       3. '返回值类型'
        Map<String,Object> = (“total”：4995，
                              genderAgg：Map("f" -> 2381,"M"->2614)，
                              detail:List(Map(一条数据)，Map(一条数据)))
```

```sql
 /**
     *
     * @param dt 查询具体哪一天的数据
     * @param keyWords 查询的关键词
     * @param aggField 分组的字段
     * @param aggCount 分组的组数
     * @param startPage 查询哪一页
     * @param sizePerPage 查询后每页的数据的数量
     * @return
     */
   Map<String,Object> getSaleDetailAndAgg(String dt,
                               String keyWords,
                               String aggField,
                               int aggCount,
                               int startPage,
                               int sizePerPage

    );
```

- 在==gmall-publisher==项目中的==service==包中==PublisherImpService实现类==中实现上面接口的抽象方法

```sql
-- 1. 分析 ： 创建一个util包，在包中创建esUtil类
   在esUtil工具类中：
   1. 创建一个获取es客户端的方法
   2. 创建一个获取dsl语句的方法
-- 2. 通过esUtil获取es的客户端和查询语句
```

```scala
package com.atguigu.gmallpublisher.utils

import io.searchbox.client.{JestClient, JestClientFactory}
import io.searchbox.client.config.HttpClientConfig
import io.searchbox.core.{Bulk, Index}

/**
  * @Description
  * @author lianzhipeng
  * @create 2020-07-20 14:58:11
  */
object EsUtil {

  /**
    *  1. 获取es的客户端
    *     a、创建一个工厂对象
    *     b、es的配置参数
    *     c、通过工厂对象 + 配置参数 创建es的对象
    */
  val factory = new JestClientFactory
  // es的url
  val esurl = "http://hadoop105:9200"
  val conf: HttpClientConfig = new HttpClientConfig.Builder(esurl)
    .connTimeout(1000 * 10) //连接延迟时间
    .readTimeout(1000 * 10) // 获取数据延迟时间
    .maxTotalConnection(100) // 最大的连接数
    .multiThreaded(true) // 是否启动多线程
    .build()

  factory.setHttpClientConfig(conf)

  def getESClient() ={
    // 创建es的客户端
    factory.getObject
  }

  def getDSL(dt : String,
             keyWords :String,
             aggField :String,
             aggCount :Int,
             startPage :Int,
             sizePerPage :Int) ={
   s"""
     |{
     |  "query": {
     |    "bool": {
     |      "filter": {
     |        "term": {
     |          "dt": ${dt}
     |        }
     |      }
     |      , "must": [
     |        {"match": {
     |          "sku_name": ${keyWords}
     |        }}
     |      ]
     |    }
     |  },
     |  "aggs": {
     |    "group_by_${aggField}": {
     |      "terms": {
     |        "field": ${aggField},
     |        "size": ${aggCount}
     |      }
     |    }
     |  },
     |  "from": ${(startPage - 1)*sizePerPage},
     |  "size": ${sizePerPage}
     |}
   """.stripMargin
  }
}

```

```sql
-- 1. 代码实现，具体的步骤：
       1. 获取es的客户端
       2. 获取查询的对象
       3. 执行查询并返回查询的结果，创建一个结果集合，用来接收这三个部分的数据
       4. 解析total数据
       5. 解析聚合的数据
            1. 创建一个集合来接收
            2. 获取聚合的数据，是一个集合
            3. 遍历集合，取出key和value，并把数据写到创建的集合中
            4. 将集合写到结果的集合中
       6. 解析订单详情的数据
            1. 创建一个集合来接收
            2. 获取聚合的数据，是一个集合
            3. 遍历集合，将一条数据作为一个整体，并把数据写到创建的集合中
            4. 将集合写到结果的集合中
       7. 返回结果集合，那么结果集合中的数据为：
          （"total"：total，
            "agg":（key：value，key，value，....),
            "detail":(Map(一条数据)，Map(一条数据)，...))
```

- 代码实现

```java
/**
     * Map(key,vaule) = (
     * “total”：4995，
     * genderAgg：Map("f" -> 2381,"M"->2614)，
     * detail:List(Map(一条数据)，Map(一条数据))
     * )
     * @param dt 查询具体哪一天的数据
     * @param keyWords 查询的关键词
     * @param aggField 分组的字段
     * @param aggCount 分组的组数
     * @param startPage 查询哪一页
     * @param sizePerPage 查询后每页的数据的数量
     * @return
     */
    @Override
    public Map<String, Object> getSaleDetailAndAgg(String dt,
                                                   String keyWords,
                                                   String aggField,
                                                   int aggCount,
                                                   int startPage,
                                                   int sizePerPage) throws IOException {
        // 1.  获取es的连接
        JestClient client = EsUtil.getESClient();
        // 2. 创建查询的对象
        // 2.1 获取dsl语句
        String dsl = EsUtil.getDSL(dt, keyWords, aggField, aggCount, startPage, sizePerPage);
        // 2.2 创建查询的对象
        Search search = new Search.Builder(dsl)
                .addIndex("gmall_sale_detail")
                .addType("_doc")
                .build();
        // 3. 执行查询，并返回查询的结果，当前的结果和kibana中显示的数据一致，我们现在需要对数据进行转换
        // 然后将转换后的数据发送给controller。
        SearchResult searchResult = client.execute(search);

        // 创建一个集合来接收最后的结果。
        HashMap<String, Object> result = new HashMap<>();
        // 4. 解析数据
        // 4.1 解析total数据
        Long total = searchResult.getTotal();
        // 将解析的数据加入到集合中
        result.put("total",total);
        // 4.2. 解析聚合数据
        // 4.2.1 创建一个集合来接收聚合数据
        HashMap<String, Long> agg = new HashMap<>();
        // 4.2.1 获取聚合的数据
        List<TermsAggregation.Entry> buckets = searchResult
                .getAggregations()
                .getTermsAggregation("group_by_" + aggField)
                .getBuckets();
        // 4.2.3 将数据的key和value取出，放进一个map集合中
        for (TermsAggregation.Entry bucket : buckets) {
            String key = bucket.getKey();
            Long value = bucket.getCount();
            agg.put(key,value);
        }
        // 4.2.4 将获取的数据写到result中
        result.put("agg",agg);
        // 4.3 解析订单详情数据
        // 4.3.1 创建一个集合来接收详情数据
        ArrayList<Map<String,Object>> detail = new ArrayList<>();
        // 4..2 获取被击中的数据
        List<SearchResult.Hit<HashMap, Void>> hits = searchResult.getHits(HashMap.class);
        // 4.2.2 遍历获取的数据
        for (SearchResult.Hit<HashMap, Void> hit : hits) {
            HashMap source = hit.source;
            detail.add(source);
        }
        // 4.2.3 将订单详情数据写到result结果中。
       result.put("detail",detail);

        // 将封装好的数据发送给controller
        return result;
    }

```

#### 2.2 通过controller对数据进行输出

```sql
-- 1. 分析：
       '1. 获取网页访问提供的参数'：
            通过网页post请求，输入指定的地址和提供指定的参数，那么在controller定义一个方法，只要这个方法处理的请求的路径和
       网页输入的地址一致，那么就能获取网页提供的参数。
       '2. 返回请求的结果'
             根据网页请求的数据（主要是指参数），对es的数据进行处理，最后将数据返回给到用户访问的页面位置。
             
             http://localhost:8070/sale_detail?dt=2019-05-20&&startPage=1&&size=5&&keyword=手机小米
-- 2. 具体的实现步骤
          1. 获取用户从网页提供的参数
          2. 对从es中获取的数据进行处理、封装、最后返回结果
          3. 用户在页面就可以看到请求的数据。
-- 3. 需返回给用户的数据
         用户需要返回的数据如下图片所示
```

![image-20200722202841153](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200722202904.png)



```

```



## 十二、小知识点

```scala
//1. json对象转换为字符串
     val context: String = obj.toJSONString
//2. json字符串转换为样例类
    JSON.parseObject(json,Classof[OrderInfo])
//3. 对象转json字符串
	Serialization.write(orderInfo)(DefaultFormats)
4. option和either的用法
```

